# python函数-问题

- ## [yeild](https://www.cnblogs.com/f-ck-need-u/p/10266226.html#!comments)


```python
for x in [s * 2 for s in "abcde"]: print(x, end=" ") 

for x in map( (lambda s: s * 2), "abcde" ): print(x, end=" ")
```

- [map](https://www.cnblogs.com/superxuezhazha/p/5714970.html)

- [lambda](https://www.cnblogs.com/evening/archive/2012/03/29/2423554.html)

```python
a=[1.0,2.0]
args=(1.0,2.0)
[a+float(b) for a,b in zip(a,args)]

#[2.0, 4.0]
```

```python
[[] for _ in range(5)]
#[[], [], [], [], []]
```

```python
(1,2)+(5,)
#(1, 2, 5)
#元组中只包含一个元素时，需要在元素后面添加逗号来消除歧义(5,)
#元组相加相当于列表的append
```

元组和字符串一样是不可变对象。不能增删改。

创建：a=()/tuple()

访问：

```python
tup1 = ('physics', 'chemistry', 1997, 2000);
tup2 = (1, 2, 3, 4, 5, 6, 7 );
 
print "tup1[0]: ", tup1[0]
print "tup2[1:5]: ", tup2[1:5]
#以上实例输出结果：
#tup1[0]:  physics
#tup2[1:5]:  [2, 3, 4, 5]
```

修改

```python
tup1 = (12, 34.56);
tup2 = ('abc', 'xyz');
 
# 以下修改元组元素操作是非法的。
# tup1[0] = 100;
 
# 创建一个新的元组
tup3 = tup1 + tup2;
print tup3;
#以上实例输出结果：
#(12, 34.56, 'abc', 'xyz')
```

- np.power:power(x, y) 函数，计算 x 的 y 次方。

- math.gamma(n):(n-1)!

eg.math.gamma(3+1)=>6

- ```python
  os.makedirs(cache_dir, exist_ok=True)
  #创建一个新的目录(cache_dir), 如果这个目录不存在的话。exist_ok=True表示如果目录已经存在，则不会发生任何错误。否则，如果exist_ok=False，如果目录已经存在，会抛出一个OSError异常。
  
  ```

- os.path.splitext()与os.path.split()

  ```python
  #os.path.splitext() 将文件名和扩展名分开
  
  #os.path.split() 返回文件的路径和文件名
  import os
   
  #os.path.join() 将分离的部分合成一个整体
  filename=os.path.join('/home/ubuntu/python_coding','split_func')
  print filename
  #输出为：/home/ubuntu/python_coding/split_func
   
  #os.path.splitext()将文件名和扩展名分开
  fname,fename=os.path.splitext('/home/ubuntu/python_coding/split_func/split_function.py')
  print 'fname is:',fname
  print 'fename is:',fename
  #输出为：
  # fname is:/home/ubuntu/python_coding/split_func/split_function
  #fename is:.py
   
  #os.path.split（）返回文件的路径和文件名
  dirname,filename=os.path.split('/home/ubuntu/python_coding/split_func/split_function.py')
  print dirname
  print filename
  #输出为：
  # /home/ubuntu/python_coding/split_func
  #split_function.py
  ```

- os.path.basename():接收一个文件路径作为参数，并返回该文件路径中的最后一个目录或文件的名称。

  ```python
  os.path.basename('/usr/local/bin/python3.7')
  
  # 'python3.7'
  ```

  ```python
  def download_extract(name, folder=None):  #@save
      """下载并解压zip/tar文件"""
      fname = download(name)
      base_dir = os.path.dirname(fname)
      data_dir, ext = os.path.splitext(fname)
      if ext == '.zip':
          fp = zipfile.ZipFile(fname, 'r')
      elif ext in ('.tar', '.gz'):
          fp = tarfile.open(fname, 'r')
      else:
          assert False, '只有zip/tar文件可以被解压缩'
      fp.extractall(base_dir)
      return os.path.join(base_dir, folder) if folder else data_dir
  
  ```

- [hashlib模块](https://blog.csdn.net/qdPython/article/details/123826017?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168008243016800225578529%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168008243016800225578529&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-123826017-null-null.142^v77^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=hashlib&spm=1018.2226.3001.4187)

  ```python
  #简单的加密方式
  import hashlib
  s = 'abc'
  md5 = hashlib.md5()    # 选择加密方式，初始化一个加密
  md5.update(s.encode('utf-8'))    # 将要加密的内容，添加到m中
  print(md5.hexdigest())
  
  ```

  

- pd.iloc[:,1,-1]切片的时候包括头不包括尾部。

  ![image-20230329180640577](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520373.png)

- [pd.concat()](https://blog.csdn.net/Mr_HHH/article/details/79488445?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168007316016800184149403%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168007316016800184149403&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79488445-null-null.142^v77^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%20pd.concat&spm=1018.2226.3001.4187):可以将数据根据不同的轴作简单的融合

  ```python
  pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,keys=None, levels=None, names=None, verify_integrity=False)
  #objs: series，dataframe或者是panel构成的序列lsit 
  frames = [df1, df2, df3]
  result = pd.concat(frames)
  
  pd.concat([df1, df4], axis=1)
  ```

  - 1

  当axis = 0的时候，concat就是列对齐，然后将不同行名称的两张表合并

  ![image-20230329183202657](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520370.png)

  

  当axis = 1的时候，concat就是行对齐，然后将不同列名称的两张表合并

  ![image-20230329183233188](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520379.png)

- [dataframe[]](https://note.youdao.com/web/#/file/recent/note/WEB1823af4e69fc01d71915da3b706b04dc/):

  ```python
  print(df[:20])  #前20行 
  print(df["Row_Labels"])#取Row_Labels这一列 
  # pandas取行或者列的注意点： 
  1.方括号中写数字，表示取行，对行进行操作 
  2.方括号中写字符串，表示取列的索引，对列进行操作。
  ```

- df.dtypes:查看数据类型,输出结果是series

- [pd.get_dummies()](https://blog.csdn.net/lindaicoding/article/details/118692358)

  ```python
  data = pd.DataFrame({"学号":[1,2,3,4],
                      "录取":["清华","北大","清华","蓝翔"],
                      "学历":["本科","本科","本科","专科"]})
  pd.get_dummies(data)
  ```

  ![img](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520433.png)

- slice():主要用在切片操作函数里的参数传递

  `class slice(start, stop[, step])`
  参数：start起始，stop终止，step步长

  ```python
  indices = slice(0, 5, 2)
  alst = [1,2,3,4,5,6,7,8,9]
  alst[indices], alst[0:5:2]
  #([1, 3, 5], [1, 3, 5])
  ```

- f的用法

  ```python
  print(f'{float(4.000)}')
  #4.0
  print(f'{float(4.000):f}')
  #4.000000默认六位小数
  print(f'{float(4.000):0.5f}')
  #4.00000
  ```

- **hasattr(object, name)**

  ```python
  object -- 对象。
  name -- 字符串，属性名。
  return
  如果对象有该属性返回 True，否则返回 False。
  
  class variable:
      x = 1
      y = 'a'
      z = True
  dd = variable() 
  print(hasattr(dd, 'x'))
  print(hasattr(dd, 'y'))
  print(hasattr(dd, 'z'))
  print(hasattr(dd, 'no'))
  结果：
  True
  True
  True
  False
  
  if not hasattr(y, "__len__"):
     #判断y是否是可迭代对象
  ```

- **isinstance(object,classinfo)**:

  用来判断一个函数是否是一个已知的类型，类似 type()

  ```python
  a = 2
  isinstance(a,int) # 结果返回 True
  ```

- **ndim**:返回的是**数组**(列表没有维度)的维度，返回的只有一个数，该数即表示数组的维度.

  ```python
  data1=[1,2,3,4]
  data2=[[1,2],[3,4]]
  a=np.array(data1)
  b=np.array(data2)
  print(a.ndim,b.ndim)
  ```

- 一张图像上画多个子图

使用plt.figure(arg)创建画板，arg为画板名称

使用plt.subplot(arg1, arg2, arg3)方法创建画纸，并选择当前画纸并绘图。其中，ag1代表第几行，arg2代表第几列，arg3代表第几个图像

1. `plt.figure()`

​      `plt.subplot(121)`

​	  `plt.plot`

2. `_, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)`

   `axes[0].plot`

   

```python
x = np.arange(0, 100, 10)

plt.figure(1)  # 生成第一个图，且当前要处理的图为fig.1

plt.subplot(1, 2, 1)  # fig.1是一个一行两列布局的图，且现在画的是左图
y1 = np.exp(x)
plt.plot(x, y1, color="r", linestyle="-", marker="^", linewidth=1)  # 画图
plt.xlabel("x")
plt.ylabel("y1")

plt.figure(1)  # 当前要处理的图为fig.1，而且当前图是fig.1的左图
plt.subplot(1, 2, 2)  # 当前图变为fig.1的右图
y2 = np.exp(1.5 * x)
plt.plot(x, y2, color="b", linestyle="-", marker="v", linewidth=1)
plt.xlabel("x")
plt.ylabel("y2")

plt.show()
```

- [OrderedDict](https://blog.csdn.net/longshaonihaoa/article/details/108469859?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168016060616800213090270%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168016060616800213090270&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-108469859-null-null.142^v77^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=OrderedDict&spm=1018.2226.3001.4187): 按照有序插入顺序存储 的有序字典

- *(1,2,3)=>1,2,3

- python中*vars代表解包元组，**vars代表解包字典，通过这种语法可以传递不定参数

- **rstrip()** 的用法 以及和 strip() 的区别

  strip() 方法用于移除字符串**头尾**指定的字符（默认为空格或换行符）或字符序列。
  rstrip() 删除 string 字符串**末尾**的指定字符（默认为空格）

- [readlines(),read()和readline()](https://blog.csdn.net/digitalkee/article/details/113620458)

  1.read()

  这个函数是把一个文档当成一个字符串（结尾有空字符串）返回。也就是一个文档存储在一个字符串对象中，如果文件很大，就很耗内存。read()和原来的文档相比，输出的时候结尾多了一个'\n'（换行）

  2.readlines()，是把一个文档的每一行（包含行前的空格，行末加一个\n），作为列表的一个元素，存储在一个list中。每一个行作为list的一个元素。readlines()和原文档相比，每行之后都多出一个空行，因为原来每行末尾都加了'\n'，要想和原文档输出一样，那么每个行末尾加上rstrip()即可消除右边空格，左边空格不变。

  3.readline()，类似于readlines()，不过它每次只读取文档的一行。以后需要逐步循环（一般用whicl循环）,最后遍历整个文件。输出情况同readlines()

- [collection](https://www.cnblogs.com/songqingbo/p/5137785.html)

  ```python
  collections.Counter(labels.values()).most_common()[-1][1]
  
  #collections.Counter(labels.values()) : 这个部分使用 collections 模块中的 Counter() 函数，对 labels.values() 中的元素进行计数。
  #.most_common() : 这个方法返回计数器中出现次数最多的元素和对应的次数，按照出现次数从大到小排序。
  #[-1] : 这个索引选取最常见元素的元组，即出现次数最少的元素和对应的次数。
  #[1] : 这个索引选取元组中的次数，即出现次数最少的元素的数量。
  ```

  ```python
  import collections
  
  obj = collections.Counter('sjndsjkdsdmslaladsldsldms')
  
  print("输出字符出现的次数字典:")
  for k,v in obj.items():
      print("{%s:%s}" % (k,v))
  
  print("输出每一个字符:") #遍历获取原始字符元素
  for k in obj.elements():
      print(k)
  
  print("输出前四个出现次数最多的字符:")
  for k in obj.most_common(4):
      print(k)
  
  输出结果：
  	输出字符出现的次数字典:
  	{s:7}
  	{l:4}
  	{m:2}
  	{d:6}
  	{k:1}
  	{n:1}
  	{j:2}
  	{a:2}
  	输出每一个字符:
  	s
  	s
  	s
  	s
  	s
  	s
  	s
  	l
  	l
  	l
  	l
  	m
  	m
  	d
  	d
  	d
  	d
  	d
  	d
  	k
  	n
  	j
  	j
  	a
  	a
  	输出前四个出现次数最多的字符:
  	('s', 7)
  	('d', 6)
  	('l', 4)
  	('m', 2)
  ```

- [dict get函数](https://www.cnblogs.com/cgmcoding/p/14428632.html)

  get() 函数返回指定键key的值value

  返回指定键的值，**如果键不在字典中返回默认值 None 或者设置的默认值。**

  ```python
  dict.get(key, default=None)
  
  #key -- 字典中要查找的键。
  #default -- 如果指定键的值不存在时，返回该默认值。
  ```

- [list extend()](https://blog.csdn.net/mz02230909mz/article/details/115704852)

  扩充列表元素内容的方法，在一定程度上其行为有点像append。只是在接受的参数以及最终的效果上有些差异。

  ```python
  list1 = [1,2,4]
  list2 = [12,6]
  list1.extend(list2)
  print(list1)
  #结果
  [1, 2, 4, 12, 6]
  
  list1 = [1,2,4]
  list2 =[[12,6],123]
  list1.extend(list2)
  print(list1)
  #结果
  [1, 2, 4, [12, 6],123]
  
  list1 = [1,2,4]
  list2 =[[12,6],123]
  list1.append(list2)
  print(list1)
  #结果
  [1, 2, 4, [[12,6], 123]]
  ```

  Extend是把每个元素都作为一个独立的元素扩充到原来的列表，而append则是把整个扩充列表作为一个元素追加到列表最后。

- ```python
  from PIL import Image
  from matplotlib import pyplot as plt
  img=plt.imread(r'C:\Users\小黑\Pictures\catdog.jpg')
  #等价于
  img=Image.open(r'C:\Users\小黑\Pictures\catdog.jpg')
  ```

  plt.imread等价于Image.open

- plt.Rectangle()

  ```python
  d2l.plt.Rectangle(xy=(400,30),width=100,height=150,fill=True,edgecolor='green',linewidth=2)
  ```

  这个函数的作用是画矩形框，通过坐标与宽高。

  这个函数的作用是画矩形框，通过坐标与宽高。
  xy=(400,30)：左上角的坐标，有了左上角坐标，然后就可以开始画了，横向往右画宽度，纵向向下画高度
  fill=True或False：表示是否填充
  edgecolor='green'：边框的颜色
  linewidth=2：画笔的大小

- fig.axes.add_patch

  fig.axes.add_patch(d2l.plt.Rectangle(xy=(20,50),width=100,height=150,fill=True,edgecolor='green',linewidth=2))

  将上面画好的矩形框，添加到画布上面，从字面意思来看，就相当于是在画布中加一块补丁

- [set_index（）](https://blog.csdn.net/Ajdidfj/article/details/123178391)

  DataFrame.set_index(*keys*, *drop=True*, *append=False*, *inplace=False*, *verify_integrity=False*)

  使用现有列设置 DataFrame 索引。

- df.iterrows()和df.items()

  df变成series

```python
import pandas as pd
df=pd.DataFrame({'A':[1,2],'B':[3,4]})
print('===df===')
print(df)
print('===df.iterrows()===')
for index,row in df.iterrows():
    print('行号：',index)
    print('该行的每列名称和值:')
    print(row)
print('===df.items()===')
for index1,column in df.items():
    print('列号:',index1)
    print('该列的每行名称和值：')
    print(column)
    
#结果：
===df===
   A  B
0  1  3
1  2  4
===df.iterrows()===
行号： 0
该行的每列名称和值:
A    1
B    3
Name: 0, dtype: int64
行号： 1
该行的每列名称和值:
A    2
B    4
Name: 1, dtype: int64
===df.items()===
列号: A
该列的每行名称和值：
0    1
1    2
Name: A, dtype: int64
列号: B
该列的每行名称和值：
0    3
1    4
Name: B, dtype: int64
```

- np.expand_dims(np,dim=X)

  用于扩展数组的形状

```python
#一维
np.expand_dims((1,2,3,4),1)
=>array([[1],
       [2],
       [3],
       [4]])
#二维
np.expand_dims([[1,2,3,4],[5,6,7,8]],0)
=>array([[[1, 2, 3, 4],
        [5, 6, 7, 8]]])

np.expand_dims([[1,2,3,4],[5,6,7,8]],1)
=>array([[[1, 2, 3, 4]],

       [[5, 6, 7, 8]]])
np.expand_dims([[1,2,3,4],[5,6,7,8]],2)
=>array([[[1],
        [2],
        [3],
        [4]],

       [[5],
        [6],
        [7],
        [8]]])
```

- [torch.permute()](https://blog.csdn.net/weixin_41377182/article/details/120808310)

交换数组的维度

- np.array().**reshape(-1)**

  torch.tensor().**reshape(-1)**

  自动把多维数组变成一维数组。

  ```python
  a=d2l.torch.tensor([[1,2,3],[4,5,6]])
  a.reshape(-1)
  =>tensor([1, 2, 3, 4, 5, 6])#变成一维
  a.reshape(1,6)
  =>tensor([[1, 2, 3, 4, 5, 6]])#二维，维度不变
  ```

- [python中dict操作集合](https://www.cnblogs.com/mxh1099/p/8512552.html)

  **创建字典的五种方法**

  ```python
  方法一: 常规方法    
  
  复制代码代码如下:
  
  # 如果事先能拼出整个字典，则此方法比较方便
  >>> D1 = {'name':'Bob','age':40}  
  
  方法二: 动态创建 
  
  复制代码代码如下:
                    
  # 如果需要动态地建立字典的一个字段，则此方法比较方便
  >>> D2 = {}  
  >>> D2['name'] = 'Bob'  
  >>> D2['age']  =  40  
  >>> D2  
  {'age': 40, 'name': 'Bob'} 
  
  方法三:  dict--关键字形式       
  
  复制代码代码如下:
  
  # 代码比较少，但键必须为字符串型。常用于函数赋值
  >>> D3 = dict(name='Bob',age=45)  
  >>> D3  
  {'age': 45, 'name': 'Bob'} 
   
  
  方法四: dict--键值序列 
  
  复制代码代码如下:
  
  # 如果需要将键值逐步建成序列，则此方式比较有用,常与zip函数一起使用
  >>> D4 = dict([('name','Bob'),('age',40)])  
  >>> D4  
  {'age': 40, 'name': 'Bob'} 
  
  或
  
  复制代码代码如下:
  
  >>> D = dict(zip(('name','bob'),('age',40)))  
  >>> D  
  {'bob': 40, 'name': 'age'}  
  
  方法五: dict--fromkeys方法# 如果键的值都相同的话,用这种方式比较好，并可以用fromkeys来初始化
  
  复制代码代码如下:
  
  >>> D5 = dict.fromkeys(['A','B'],0)  
  >>> D5  
  {'A': 0, 'B': 0}  
  
  如果键的值没提供的话，默认为None
  
  复制代码代码如下:
  
  >>> D3 = dict.fromkeys(['A','B'])  
  >>> D3  
  {'A': None, 'B': None}  
  ```


- plt: subplot()、subplots()详解及返回对象figure、axes的理解 [here](https://blog.csdn.net/sunjintaoxxx/article/details/121098302?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168601496616782427472781%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168601496616782427472781&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121098302-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=plt.subplots&spm=1018.2226.3001.4187)
- VSCode远程调试 ：Linux/Ubuntu远程服务器使用plt.show()没有反应

**解决方法：保存成png图片然后在程序运行后查看**

```python
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
img = np.array([[1, 2], [3, 4]])
plt.imshow(img)
plt.savefig("/home/img_save_folder/Picture.png")  
#这里是要保存的路径/home/img_save_folder/和保存文件名Picture.png
```

如果要同时保存多个图片，为防止覆盖，可以如下添加plt.close()以及为图片名编号：

```python
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
for i in range(5):
	img = np.random.randint(10,100,size=(4,3)) #4行3列，每个元素是从10~99之间的随机数)
	plt.imshow(img)
	plt.savefig("/home/img_save_folder/Picture"+str(i)+".png")
	#这里是要保存的路径/home/img_save_folder/和保存文件名Picture0.png、Picture1.png...
    plt.close()
```

- [re.sub()](https://blog.csdn.net/jackandsnow/article/details/103885422?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168870665116800222830201%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168870665116800222830201&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-103885422-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=re.sub&spm=1018.2226.3001.4187)

`re.sub(r'[A-Za-z]+', '*', s)` 这句话则表示匹配多个连续的字母，并将**多个连续的字母替换为一个星号**

`re.sub(r'[^A-Za-z]+', '*', s)` 这句话则表示匹配多个连续的非字母，并将**多个连续的非字母替换为一个星号** 

- split()

string.split(str, max)

str – 分隔符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。
max – 分割次数。规定要执行的拆分数。默认值为 -1，即“所有出现次数”。

```python
#例1：
>>> a="I love China"
>>> a.split()   # 分隔符为空，分割次数默认
['I', 'love', 'China']
>>> 
#例2：
>>> b="I love China, and you, you"
>>> b.split(", ")    # 使用逗号和空格作为分隔符
['I love China', 'and you', 'you']
>>> 
#例3：
>>> c="I#love#China#andyou#you"
>>> c.split("#")   #使用#作为分隔符
['I', 'love', 'China', 'andyou', 'you']
>>> 
#例4：
>>> d="I#love#China#andyou#you"
>>> d.split("#",1)   # 将 max值为 1，将返回包含 2 个元素的列表
['I', 'love#China#andyou#you']
>>> 
#例5：
>>> e="with great power comes great responsibility. I love China and you you"
>>> e.split(" ",15) #空格为分隔符
['with', 'great', 'power', 'comes', 'great', 'responsibility.', 'I', 'love', 'China', 'and', 'you', 'you']
>>> 
总结：
（1）split()有两个参数，第一个参数是分隔符，如果不指定，则默认以空格、换行、制表符为分隔符，第二个参数为分隔次数，如果不指定，则跟据字符串中有多少个分隔符，就分隔多少次。
（2）例4字符串d指定井号'#'为分隔符，只分隔1次，所以在I和love之间分隔一次，后面的都不分隔。
（3）例5字符串e这个字符串中总共有15个空格，如果指定分隔次数大于15，则Python也不会报错，还是以分隔符的总数进行分隔。

```

- 排序函数sorted()

sorted(iterable, cmp=None, key=None, reverse=False)

【iterable】 可迭代对象。

【cmp】 比较的函数，这个具有两个参数，参数的值都是从可迭代对象中取出，此函数必须遵守的规则为，大于则返回1，小于则返回-1，等于则返回0。（一般省略）

【key】主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。
常用的用来作为参数key的函数有 lambda函数和operator.itemgetter()
尤其是列表元素为多维数据时，需要key来选取按哪一位数据来进行排序

【reverse】 排序规则，reverse = True 降序 ， reverse = False 升序（默认）。

```python
list = ['a', 'bc', 'defg', 'handsome', 'qwerrtyyuu']
print(sorted(list,key=lambda x:len(x),reverse=True))

Output：
['qwerrtyyuu', 'handsome', 'defg', 'bc', 'a']
```

- pickle [here](https://blog.csdn.net/qq_28790663/article/details/115496733?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168906069716782427473989%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168906069716782427473989&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115496733-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=pickle&spm=1018.2226.3001.4187)

dump()函数

作用：**实现python对象的序列化，将obj保存到file中**

dumps()函数

作用：返回一个字符串，而不存入文件

load()函数

作用：用于[反序列化](https://so.csdn.net/so/search?q=反序列化&spm=1001.2101.3001.7020)，将序列化的对象重新恢复成python对象

loads()函数

作用：从字符串中恢复对象。

- sklearn.processing     **LabelEncoder**  [here](https://blog.csdn.net/zengbowengood/article/details/120778942?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168906363316800182751563%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168906363316800182751563&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-120778942-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=LabelEncoder&spm=1018.2226.3001.4187)

~~~python
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
encoder.fit(["paris", "paris", "tokyo", "amsterdam"]) 

转换及输出
```python
encoder.transform(["tokyo", "tokyo", "paris"])
array([2, 2, 1], dtype=int64)

list(encoder.inverse_transform([2, 2, 1]))
['tokyo', 'tokyo', 'paris']

aoc_encoder = LabelEncoder() #机型编码
raw_data["AC_TYPE_LONG"] = aoc_encoder.fit_transform(raw_data["AC_TYPE_LONG"]).astype('float32')

airport_encoder = LabelEncoder() #机场编码
raw_data["AIRPORT_4CODE"] = airport_encoder.fit_transform(raw_data["AIRPORT_4CODE"]).astype('float32')

aoc_encoder = LabelEncoder() #机型编码
raw_data["AC_TYPE_LONG"] = aoc_encoder.fit_transform(raw_data["AC_TYPE_LONG"]).astype('float32')
aoc_type_encoder = open(r"D:\三行航空物流\项目\最大起飞重量限制\aoc_type_encoder.pickle", "wb")
pickle.dump(aoc_encoder, aoc_type_encoder) #保存机型编码模型
aoc_type_encoder.close()

airport_encoder = LabelEncoder() #机场编码
raw_data["AIRPORT_4CODE"] = airport_encoder.fit_transform(raw_data["AIRPORT_4CODE"]).astype('float32')
airport_type_encoder = open(r"D:\三行航空物流\项目\最大起飞重量限制\airport_type_encoder.pickle", "wb")
pickle.dump(airport_encoder, airport_type_encoder) #保存机场编码模型
airport_type_encoder.close()

aoc_pkl = open(r"D:\三行航空物流\项目\最大起飞重量限制\aoc_type_encoder.pickle", 'rb') #机型反编码
encoder = pickle.load(aoc_pkl) 
aoc_pkl.close()
X1 = encoder.transform(['A320-214'])[0]
print(X1)

airport_pkl = open(r"D:\三行航空物流\项目\最大起飞重量限制\airport_type_encoder.pickle", 'rb') #机场反编码
encoder = pickle.load(airport_pkl) 
airport_pkl.close()
X2 = encoder.transform(['EDHI'])[0]
print(X2)
~~~



# pytorch

TensorDataset():把输入的两类数据进行一 一对应；

DataLoader：重新排序

torchvision数据集下载位置修改

1.torchvision.datasets:一些加载数据的函数及常用的数据集接口；
2.torchvision.models:包含常用的模型结构，例如AlexNet，VGG，ResNet等；
3.torchvision.transforms:常用的一些图片变换，例如图片裁剪、选择等；
4.torchvison.utils:其他一些有用的方法

```python
trans = transforms.ToTensor()
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)



#其中，
root：表示数据集下载保存位置
train：表示下载的数据集是不是训练集，True表示训练集，False表示测试集
download：表示数据集是否需要下载
transform：表示图片变换的一系列操作

root='/'：表示根目录下，如果你的代码保存在D盘，就是下载到D盘根目录下
root='./'：表示当前文件夹下
root=''：效果等同于'./'
root='./data'：表示在当前文件夹下的data（如果没有，则会新建一个）文件夹下保存数据集
root='data'：效果等同于'./data'
root='../':表示上一层目录
root='../../':表示上上一层目录
```

```python
trans = [transforms.ToTensor()]
#trans是包括transforms.ToTensor()的列表
trans.insert(0, transforms.Resize(resize))
trans = transforms.Compose(trans)
##transforms.Compose（[.. , .. , ..]）
```

```python
y=torch.Tensor([1.0,2.0])
y.numel()

a.numel()是一个 PyTorch 方法, 它用于计算元素数量，即张量（Tensor）中元素的总数。在这个代码中，变量 "y" 应该是一个张量，这个方法就是用来统计 y 张量中的元素个数的。
```

```python
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
    
    这三行代码的功能是进行断言检查，确保训练和测试的精度和损失符合要求。如果断言不成立，则会抛出AssertionError异常。
    
具体来说：
	assert train_loss < 0.5, train_loss：确保训练损失小于0.5，否则抛出异常，并打印出train_loss的值。
	assert train_acc <= 1 and train_acc > 0.7, train_acc：确保训练精度在0.7到1.0之间，否则抛出异常，并打印出train_acc的值。
	assert test_acc <= 1 and test_acc > 0.7, test_acc：确保测试精度在0.7到1.0之间，否则抛出异常，并打印出test_acc的值。
	这样做的目的是为了确保模型的训练和测试结果是合理的，不会出现特别低或特别高的情况。
```

```python
backward(retain_graph=True)

retain_graph==True:进行一次backward之后，各个节点的值会清除，这样进行第二次backward会报错，如果加上retain_graph==True后,可以再来一次backward。
```

- 训练步骤： 1.建立网络 2. 损失函数 3. 优化器（根据反向传播求得梯度 用优化器更具体都来更新参数） 4. 从训练集取出数据，进行训练，先梯度清0，算损失，反向传播，然后优化

- ```python
  weight_decay
   trainer = torch.optim.SGD([
          {"params":net[0].weight,'weight_decay': wd},
          {"params":net[0].bias}], lr=lr)
  
  #第一个元素{"params":net[0].weight,'weight_decay': wd}表示要优化的参数为网络net的第一个层的权重参数，同时还设置了L2正则化（weight decay）的参数为wd。第二个元素{"params":net[0].bias}表示要优化的参数为网络net的第一个层的偏置参数。
  ```

- torch.rand和torch.randn

  torch.rand均匀分布默认[0,1）

  torch.randn标准正态分布,，默认均值为0，方差为1

- torch.clamp():

  torch.clamp()等价于torch.clip()，用于设置 input tensor 在 min 和 max 之间。如果 input tensor 中元素的值小于 min，则设置该元素值为 min，如果 input tensor 中元素的值大于 max，则设置该元素值为 max。如果 min 大于 max，则将元素值设置为 max。如果 min 为None，则无下界，如果 max 为 None，则无上界。

```python
torch.clamp(input, min=None, max=None) → Tensor
Tensor.clamp(min=None, max=None) → Tensor
torch.clip(input, min=None, max=None) → Tensor
Tensor.clip(min=None, max=None) → Tensor
```

- torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)

  `params(iterable)：可用于迭代优化的参数或者定义参数组的dicts。`
  `lr (float, optional) ：学习率(默认: 1e-3)，更新梯度的时候使用`
  `betas (Tuple[float, float], optional)：用于计算梯度的平均和平方的系数(默认: (0.9, 0.999))`
  `eps (float, optional)：为了提高数值稳定性而添加到分母的一个项(默认: 1e-8)`
  `weight_decay (float, optional)：权重衰减(如L2惩罚)(默认: 0)，针对最后更新参数的时候，给损失函数中的加的一个惩罚参数，更新参数使用`

- **net.add_module(name, module)**

  name是添加模块的名称，module是要添加的子模块。在PyTorch中，每个模块都应该被命名，以便在不同的地方使用时能够区分。使用add_module方法添加的模块将会被添加到网络（net）中，并且可以通过名称进行访问。这使得网络更易于管理和调试。

- **m.weight.data *= m.weight.data.abs() >= 5**

  这里的运算符顺序是先判断大于等于得到布尔值再和左边相乘,因此这句话的意思是先判断m.weight.data是否大于5，得到F/T布尔值，对应0/1，再乘以m.weight.data。

- [torch.nn.Parameter](https://blog.csdn.net/weixin_44966641/article/details/118730730?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168016453916800182113998%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168016453916800182113998&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-118730730-null-null.142^v77^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=nn.parameter&spm=1018.2226.3001.4187)

  将一个不可训练的类型`Tensor`转换成可以训练的类型`parameter`并将这个`parameter`绑定到这个`module`里面(`net.parameter()`中就有这个绑定的`parameter`，所以在参数优化的时候可以进行优化

- **torch.matmul**可以用在更多维度的矩阵运算，

  **torch.mm**只能在两纬

- **model.zero_grad()和optimizer.zero_grad()**


```python
model.zero_grad()
optimizer.zero_grad()
```

首先，这两种方式都是把模型中参数的梯度设为0

当optimizer = optim.Optimizer(net.parameters())时，二者等效，其中Optimizer可以是Adam、SGD等优化器

```python
def zero_grad(self):
        """Sets gradients of all model parameters to zero."""
        for p in self.parameters():
            if p.grad is not None:
                p.grad.data.zero_()

```

- [**torch.stack**](https://blog.csdn.net/chensen99/article/details/124934391)

  是一个 PyTorch 中的函数，用于把多个张量（Tensor）按照给定的维度进行堆叠，返回一个新的张量。

  需要传入一个张量序列，以及一个整数 dim 或者axis参数。默认dim=0

  ```python
  a = torch.randn(2, 3)
  b = torch.randn(2, 3)
  
  c = torch.stack([a, b], dim=0)
  
  print(c.shape)
  
  #torch.Size([2, 2, 3])
  ```

  ```python、
  T1 = torch.tensor([[1, 2, 3],
          		[4, 5, 6],
          		[7, 8, 9]])
  T2 = torch.tensor([[10, 20, 30],
          		[40, 50, 60],
          		[70, 80, 90]])
  print(torch.stack((T1,T2),dim=0))#(2,3,3)，
  print(torch.stack((T1,T2),dim=1))
  print(torch.stack((T1,T2),dim=2))
  #
  tensor([[[ 1,  2,  3],
           [ 4,  5,  6],
           [ 7,  8,  9]],
  
          [[10, 20, 30],
           [40, 50, 60],
           [70, 80, 90]]])
  
  tensor([[[ 1,  2,  3],
           [10, 20, 30]],
  
          [[ 4,  5,  6],
           [40, 50, 60]],
  
          [[ 7,  8,  9],
           [70, 80, 90]]])
  
  tensor([[[ 1, 10],
           [ 2, 20],
           [ 3, 30]],
  
          [[ 4, 40],
           [ 5, 50],
           [ 6, 60]],
  
          [[ 7, 70],
           [ 8, 80],
           [ 9, 90]]])
  ```

- [torch.cat() 和 torch.stack()的区别](https://blog.csdn.net/hello_program_world/article/details/114340833?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168153904616800180659786%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168153904616800180659786&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-114340833-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=torch.cat%28%29%E5%92%8Ctorch.stack%28%29&spm=1018.2226.3001.4187)

1. torch.stack()是新增一维，属于增维操作（2*2，2*2 → 2*2*2）。

2. torch.cat()是在特定维度上进行拼接（2*2， 2*2 → 2*4）。

   ```python
   #(a1,a2)可以换成[a1,a2]
   a0=torch.Tensor([[[[1,1,1,1],[2,2,2,2]]]])
   a1=torch.Tensor([[[[3,3,3,3],[4,4,4,4]]]])
   torch.cat((a0,a1),dim=3).type(torch.FloatTensor)
   tensor([[[[1., 1., 1., 1., 3., 3., 3., 3.],
             [2., 2., 2., 2., 4., 4., 4., 4.]]]])
   torch.Size([1, 1, 2, 8])
   ```
   
   

- 通过卷积层学习图片的空间信息；

  通过池化层降低图片信息敏感度；

   通过全连接层转换到类别空间。

- [self.__class__.__name__](https://blog.csdn.net/Yy_Rose/article/details/123549727?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168051294616800182757522%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168051294616800182757522&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-123549727-null-null.142^v81^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=.__class__.__name__&spm=1018.2226.3001.4187)

  **作用：**获取当前类的类名

  **原理：**首先用 __class__ 将 self实例变量指向类，然后再去调用 __name__ 类属性

- torch.numel()

  ```python
  t = torch.randn((2,3,4))
  t.numel()
  #24
  ```

- 要学习的参数个数：

每个卷积层参数：co*ci*kh*kw+co(bias),全连接层参数就多了，卷积后图像拉直送入全连接层

- [**下采样和上采样**](https://blog.csdn.net/ytusdc/article/details/121452878?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168057663916800211584736%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168057663916800211584736&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121452878-null-null.142^v81^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%E4%B8%8A%E9%87%87%E6%A0%B7%E5%92%8C%E4%B8%8B%E9%87%87%E6%A0%B7&spm=1018.2226.3001.4187)

  1、下采样的作用是什么？通常有哪些方式？
  	下采样层有两个作用，一是减少计算量，防止过拟合；二是增大感受野，使得后面的卷积核能够学到更加全局的信息。

  下采样的方式主要有两种：
  		1、采用stride为2的池化层，如Max-pooling和Average-pooling，目前通常使用Max-pooling，因为他计算简单而且能够更好的保留纹理特征；
  		2、采用stride为2的卷积层，下采样的过程是一个信息损失的过程，而池化层是不可学习的，用stride为2的可学习卷积层来代替pooling可以得到更好的效果，当然同时也增加了一定的计算量。

  2、上采样的原理和常用方式

  在卷积神经网络中，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(如图像的语义分割)，这个使图像由小分辨率映射到大分辨率的操作，叫做上采样，它的实现一般有三种方式：

  ​		1、插值，一般使用的是双线性插值，因为效果最好，虽然计算上比其他插值方式复杂，但是相对于卷积计算可以说不值一提，其他插值方式还有最近邻插值、三线性插值等；

  ​		2、转置卷积又或是说反卷积(Transpose Conv)，通过对输入feature map间隔填充0，再进行标准的卷积计算，可以使得输出feature map的尺寸比输入更大；是一种可以学习的向上采样方式，里面的参数可以学习

  nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')
  		3、Up-Pooling - Max Unpooling && Avg Unpooling --Max Unpooling，在对称的max pooling位置记录最大值的索引位置，然后在unpooling阶段时将对应的值放置到原先最大值位置，其余位置补0；没有参数，速度更快，采取给定策略上采样

- **nn.AdaptiveAvgPool2d**

  AdaptivePooling，自适应池化层。使用这种池化方式，核（kernal）和步长（stride）是函数根据输入的原始尺寸、目标尺寸**自动计算**出来的。

  ```python
  *# 自适应最大池化* 
  
  out=nn.AdaptiveAvgPool2d(output_size=100)
  ```

  构造模型的时候，AdaptiveAvgPool2d()的位置一般在卷积层和全连接层的交汇处，以便确定输出到Linear层的大小。

- 卷积核的组数是输出通道，每组有多少个卷积核是输入通道

- 批量归一化：保证层间输出和梯度符合某一特定分布，以提高数据和损失的稳定性

- 网络常用设计思路：

  高宽减半，通道加倍

- nn.Conv2d和F.conv2d

- ```python
  nn.Conv2d(in_chanels,out_chanels,kernel_size=3,stride=1,padding=0)
  x=(batch,in_chanels,n_h,n_w)
  out=(batch,out_chanels,m_h,m_w)
  w=(out_chanels,int_chanels,k_h,k_w)
  b=(out_chanels)
  
  F.conv2d(x,w,b,stride=,padding=) 
  #x,w,b,y的结构都是一样的
  ```

  ![image](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520315.jpeg)

​					**图片中的偏差B是C0，不是C0*Ci**

- 卷积原理：

![image-20230613155015868](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306131959406.png)

- 卷积核的输出形状计算公式：

  <img src="https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041632048.png" alt="image-20230407154849459" style="zoom: 200%;" />

- torch.mm只适用于二维矩阵，而torch.matmul可以适用于高维

- 图像增广：翻转，切割，变色

- detach() 、detach_()和 data 的区别

  1）detach()与detach_()

  ```
  在x->y->z传播中，如果我们对y进行detach()，梯度还是能正常传播的，但如果我们对y进行detach_()，就把x->y->z切成两部分：x和y->z，x就无法接受到后面传过来的梯度
  
  2）detach()和data
  
  共同点：x.data（x.detach()） 返回和 x 的相同数据 tensor, 这个新的tensor和原来的tensor（即x）是共用数据的，一者改变，另一者也会跟着改变，且require s_grad = False
  
  不同点： x.data 不能被 autograd 追踪求微分，即使被改了也能错误求导，而x.detach()则不行
  ```

- [torchvision.datasets.ImageFolder](https://blog.csdn.net/weixin_54546190/article/details/126123675)

  ```python
  train_imgs=torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))
  print(train_imgs.classes)
  print(train_imgs.class_to_idx)
  print(train_imgs[0])#结构是(img_data, class_id),带标签
  print(train_imgs[0][0])#是img_data
  
  #结果：
  ['hotdog', 'not-hotdog']
  {'hotdog': 0, 'not-hotdog': 1}
  (<PIL.Image.Image image mode=RGB size=122x144 at 0x2844AFD7648>, 0)
  <PIL.Image.Image image mode=RGB size=122x144 at 0x2844B716808>
  ```

- [训练集、验证集与测试集](https://blog.csdn.net/mooyuan/article/details/123544318)

训练集（train set） —— 用于训练模型（拟合参数）：即模型拟合的数据样本集合，如通过训练拟合一些参数来建立一个分类器。

验证集（validation set）—— 用于确定网络结构或者控制模型复杂程度的超参数（拟合超参数）：是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。 通常用来在模型迭代训练时，用以验证当前模型泛化能力（准确率，召回率等），防止过你话的现象出现，以决定如何调整超参数。具体原理参照本文的二（三）。

测试集（test set） —— 用来评估模最终模型的性能如何（评价模型好坏）：测试集没有参于训练，主要是测试训练好的模型的准确能力等，但不能作为调参、选择特征等算法相关的选择的依据。说白了就只用于评价模型好坏的一个数据集。

- [torch.optim.lr_scheduler.StepLR](https://blog.csdn.net/weixin_44543648/article/details/122623774)

  学习率调整

  ```python
  torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=- 1, verbose=False)
  
  #optimizer：要优化的优化器
  #step_size：每训练step_size个epoch，更新一次参数
  #gamma：更新lr的乘法因子
  ```

  ![在这里插入图片描述](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520687.png)

- **torch.set_printoptions**(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)

  **修改输出精度**

  precision是每一个元素的输出精度，默认是八位；
  threshold是输出时的阈值，当tensor中元素的个数大于该值时，进行缩略输出，默认时1000；
  edgeitems是输出的维度，默认是3；
  linewidth字面意思，每一行输出的长度；
  profile=None，修正默认设置

- opencv,PIL,numpy和torch读取数据格式及转化

**1.PIL的Image.open()读取图片**

PIL图像在转换为numpy.ndarray后，格式为(h,w,c)，像素顺序为RGB**；**

**2.opencv的imread读取图像**

OpenCV在cv2.imread()后数据类型为numpy.ndarray，格式为(h,w,c)，像素顺序为BGR；

**3.torchvision.transforms.ToTensor()**

**源码说明：将PIL image或者一个numpy.ndarray变成tensor**

**当PIL image在**(L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)之间

当numpy.adarray类型为np.uint8

**输入应该是numpy.ndarray,维度是（H,W,C）,数值【0-255】**

**输出变成torch.FloatTensor,维度是（C,H,W）,数值在【0.0-1.0】之间**

可以用permute函数进行维度转化

```py
ndarray=>tensor   torchvision.transforms.ToTensor()  
tensor=>ndarray   tensor.permute(0,2,3,1)
```

- [torch.meshgrid(*tensors,indexing=None)](https://blog.csdn.net/flyingluohaipeng/article/details/125033606?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168153781316800182179206%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168153781316800182179206&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-125033606-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=torch.meshgrid&spm=1018.2226.3001.4187)

函数的功能是生成网格，可以用于生成坐标。

1输入两个一维tensor数据，且两个tensor数据类型相同

2输出两个tensor数据

3参数indexing是可选的，可选值为’‘xy’‘或’‘ij’‘，默认为’‘ij’'。

如果选择**“ij”**，则输出两个tensor维度的行列个数与输入的两个tensor的元素个数分别对应相同，如上面例子所示。
如果选择’**‘xy’'**，则输出两个tensor的第一个维度对应于第二个输入的元素个数，第二个维度对应于第一个输入的元素个数。其中第一个输出张量填充第一个输入张量中的元素，各列元素相同；第二个输出张量填充第二个输入张量中的元素，各行元素相同，与indexing='‘ij’'输出结果情况相反

```python
x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5])
grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')
print(x)
print(y)
print(grid_x)
print(grid_y)
结果如下：
tensor([1, 2, 3])
tensor([4, 5])
tensor([[1, 1],
        [2, 2],
        [3, 3]])
tensor([[4, 5],
        [4, 5],
        [4, 5]])
```

```python
x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5])
grid_x, grid_y = torch.meshgrid(x, y, indexing='xy')
print(x)
print(y)
print(grid_x)
print(grid_y)
结果如下所示：
tensor([1, 2, 3])
tensor([4, 5])
tensor([[1, 2, 3],
        [1, 2, 3]])
tensor([[4, 4, 4],
        [5, 5, 5]])
```

- [pytorch中repeat()函数](https://blog.csdn.net/tequila53/article/details/119183678?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168154229116800188516713%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168154229116800188516713&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-119183678-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=repeat%E5%87%BD%E6%95%B0&spm=1018.2226.3001.4187)

对数组进行重复。

```python
a = torch.tensor([[1, 2, 3],
                  [1, 2, 3]])
b = a.repeat(2, 2)
print(b.shape)
print(b)
=>torch.Size([4, 6])
tensor([[1, 2, 3, 1, 2, 3],
        [1, 2, 3, 1, 2, 3],
        [1, 2, 3, 1, 2, 3],
        [1, 2, 3, 1, 2, 3]])
#repeat的参数是对应维度的复制个数，上段代码为0维复制两次，1维复制两次，则得到以上运行结果。其余扩展情况依此类推


# a形状(2,3)
a = torch.tensor([[1, 2, 3],
                  [1, 2, 3]])
# repeat参数比维度多，在扩展前先讲a的形状扩展为(1,2,3)然后复制
b = a.repeat(1, 2, 1)
print(b.shape)  # 得到结果torch.Size([1, 4, 3])
```

- [张量复制方法repeat、repeat_interleave和tile](https://blog.csdn.net/bqw18744018044/article/details/127481387?ops_request_misc=&request_id=&biz_id=102&utm_term=repeat_interleave&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-127481387.142^v83^insert_down38,239^v2^insert_chatgpt&spm=1018.2226.3001.4187)

- [torch.unsqueeze()和torch.squeeze()](https://blog.csdn.net/flyingluohaipeng/article/details/125092937?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168154439916800225519146%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168154439916800225519146&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-125092937-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=unsqueeze&spm=1018.2226.3001.4187)

`torch.squeeze(input, dim=None, out=None)` 

squeeze()函数的功能是维度压缩。返回一个tensor（张量），其中 input 中维度大小为1的所有维都已删除。
举个例子：如果 input 的形状为 (A×1×B×C×1×D)，那么返回的tensor的形状则为 (A×B×C×D)
当给定 dim 时，那么只在给定的维度（dimension）上进行压缩操作，注意给定的维度大小必须是1，否则不能进行压缩。
`torch.unsqueeze(input, dim) → Tensor`

unsqueeze()函数起升维的作用,参数dim表示在哪个地方加一个维度，注意dim范围在:[-input.dim() - 1, input.dim() + 1]之间，比如输入input是一维，则dim=0时数据为行方向扩，dim=1时为列方向扩，再大错误。

- [torch.max()](https://blog.csdn.net/flyingluohaipeng/article/details/125093651?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168156584316800222862877%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168156584316800222862877&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-125093651-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=torch.max&spm=1018.2226.3001.4187)同torch.min

1.torch.max(input)

```python
1.输入一维张量，返回一维张量里面最大值
x = torch.randn(4)
y = torch.max(x)
x,y
输出结果如下：
(tensor([-0.6223,  0.0043, -0.8753,  1.4240]), tensor(1.4240))

2 输入二维张量，返回二维张量里面最大值
x = torch.randn(3,4)
y = torch.max(x)
x,y
输出结果如下：
(tensor([[-1.1052,  0.1026,  0.9994, -0.3092],
         [-0.8400,  0.2004,  0.9212,  0.7807],
         [-1.2979, -0.4327,  2.3044,  0.0140]]),
 tensor(2.3044))

3 输入两个一维张量，输出这两个张量里面相应元素中的最大值
x = torch.randn(4)
z = torch.randn(4)
max = torch.max(x,z)
x,z,max
输出结果如下：
(tensor([-1.5147, -1.2790, -1.0159, -0.4732]),
 tensor([-0.4547, -2.8545,  0.0554, -0.3548]),
 tensor([-0.4547, -1.2790,  0.0554, -0.3548]))

4 输入两个张量，一个张量一维，一个张量二维，此时一维张量会进行广播成二维张量，然后再输出这两个张量里面相应元素中的最大值，输出张量为二维。
x = torch.randn(3,4)
z = torch.randn(4)
max = torch.max(x,z)
x,z,max
输出结果如下：
(tensor([[ 1.1917,  0.6338,  0.7590, -0.9802],
         [ 0.2247,  0.3635,  1.3743,  1.6229],
         [ 1.6165,  0.0634,  0.5259,  0.1285]]),
 tensor([3.4765, 0.4480, 0.1502, 0.3738]),
 tensor([[3.4765, 0.6338, 0.7590, 0.3738],
         [3.4765, 0.4480, 1.3743, 1.6229],
         [3.4765, 0.4480, 0.5259, 0.3738]]))

5 输入两个二维张量，输出这两个张量里面相应元素中的最大值，输出张量为二维。
x = torch.randn(3,4)
z = torch.randn(3,4)
max = torch.max(x,z)
x,z,max
输出结果如下：
(tensor([[-0.0835,  0.0718, -1.7404, -0.3218],
         [ 0.0577,  0.6271,  1.4014, -0.6417],
         [ 0.3917,  0.0761,  1.2479, -0.4352]]),
 tensor([[-0.0717,  0.3822,  0.7256,  1.4147],
         [-0.1271,  0.1503,  0.3934,  1.6760],
         [-2.2341,  2.5286, -0.3500, -0.1751]]),
 tensor([[-0.0717,  0.3822,  0.7256,  1.4147],
         [ 0.0577,  0.6271,  1.4014,  1.6760],
         [ 0.3917,  2.5286,  1.2479, -0.1751]]))
```

2.torch.max(input,dim=x)

输入input（二维）张量，

当dim=0时表示找出每列的最大值，函数会返回两个tensor，第一个tensor是每列的最大值，第二个tensor是每列最大值的索引；

当dim=1时表示找出每行的最大值，函数会返回两个tensor，第一个tensor是每行的最大值；第二个tensor是每行最大值的索引。

```python
x = torch.randn(3,4)
max,indices = torch.max(x,dim=1)
x,max,indices
输出结果如下：
(tensor([[ 1.4832,  0.1886, -0.3044, -0.6111],
         [-0.8998,  0.0610,  0.3388,  1.7176],
         [ 1.6153,  0.6864,  2.3225,  1.3818]]),
 tensor([1.4832, 1.7176, 2.3225]),
 tensor([0, 3, 2]))
```

- [torch.argmax()](https://blog.csdn.net/flyingluohaipeng/article/details/125099214)

1.`torch.argmax(input) → LongTensor`

**将输入input张量，无论有几维，首先将其reshape排列成一个一维向量，然后找出这个一维向量里面最大值的索引。**

```python
import torch
x = torch.randn(3,4)
y = torch.argmax(x)#对应于x中最大元素的索引值
x,y
(tensor([[-0.84,  1.66,  0.60, -0.49],
         [ 0.89,  1.16,  1.40, -0.45],
         [-0.83,  0.06,  0.26,  0.67]]),
 tensor(1))
```

2.`torch.argmax(input, dim, keepdim=False) → LongTensor`

**函数返回其他所有维在这个维度上面张量最大值的索引。**

```python
import torch
x = torch.randn(3,4)
y = torch.argmax(x,dim=0)#dim=0表示将dim=0这个维度大小由3压缩成1，然后找到dim=0这三个值中最大值的索引，这个索引表示dim=0行索引标号
x,x.shape,y,y.shape
输出结果如下：
(tensor([[ 2.6347,  0.6456, -1.0461, -1.5154],
         [-1.3955, -1.2618, -0.5886, -0.5947],
         [-1.5272, -2.0960,  0.9428, -0.9532]]),
 torch.Size([3, 4]),
 tensor([0, 0, 2, 1]),
 torch.Size([4]))
```

- [torch.sort()和torch.argsort()](https://blog.csdn.net/flyingluohaipeng/article/details/125093568)

1.torch.sort()

`torch.sort(input, dim=- 1, descending=False, stable=False, *, out=None)`

**输入input，在dim维进行排序，默认是dim=-1对最后一维进行排序，descending表示是否按降序排,默认为False，输出排序后的值以及对应值在原输入imput中的下标**

```python
x = torch.randn(3, 4)
sorted, indices = torch.sort(x)
x,sorted,indices
输出结果如下：
(tensor([[-1.3864,  0.5811, -0.1056, -0.3237],
         [-0.2136, -1.4806,  0.4986,  0.9382],
         [-0.2820,  0.1171, -0.3983, -0.8061]]),
 tensor([[-1.3864, -0.3237, -0.1056,  0.5811],
         [-1.4806, -0.2136,  0.4986,  0.9382],
         [-0.8061, -0.3983, -0.2820,  0.1171]]),
 tensor([[0, 3, 2, 1],
         [1, 0, 2, 3],
         [3, 2, 0, 1]]))
```

2.torch.argsort()

`torch.argsort(input,dim=-1,descending=False)`

- pytorch clamp()

在最大值和最小值之间对input进行裁剪.

```python
torch.clamp(input, min, max, out=None)→ Tensor
or a.clamp(min,max)
```

![image-20230415223050119](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520133.png)

- [[:,None]用于增加维度](https://blog.csdn.net/flyingluohaipeng/article/details/125093683?spm=1001.2014.3001.5506)

```python
#三维变四维
a=torch.tensor([[[1,2,3],
                 [4,5,6]],
   				[[1,2,3],
                 [4,5,6]]])
print(a[None,:,:,:])
print(a[:,None,:,:])#None在第一维和第三维之间就扩展第二维
print(a[:,:,None,:])
print(a[:,:,:,None])
=>
tensor([[[[1, 2, 3],
          [4, 5, 6]],

         [[1, 2, 3],
          [4, 5, 6]]]])
tensor([[[[1, 2, 3],
          [4, 5, 6]]],


        [[[1, 2, 3],
          [4, 5, 6]]]])
tensor([[[[1, 2, 3]],

         [[4, 5, 6]]],


        [[[1, 2, 3]],

         [[4, 5, 6]]]])
tensor([[[[1],
          [2],
          [3]],

         [[4],
          [5],
          [6]]],


        [[[1],
          [2],
          [3]],

         [[4],
          [5],
          [6]]]])
```

```python
#二维变三维，广播机制
a=torch.tensor([[1,2,3,4],
                [4,5,6,7]])#(2,4)
b=torch.tensor([[1,2,3,4],
                [5,6,7,8],
                [5,6,7,8]])#(3,4)
print(a[None,:,:])#(1,2,4)
print(a[:,None,:])#(2,1,4)
print(a[:,:,None])#(2,4,1)
print(torch.max(a[:,None,:],b))#(2,3,4)
print(a[:,None,0])#二维(2,1)
=>
tensor([[[1, 2, 3, 4],
         [4, 5, 6, 7]]])
tensor([[[1, 2, 3, 4]],

        [[4, 5, 6, 7]]])
tensor([[[1],
         [2],
         [3],
         [4]],

        [[4],
         [5],
         [6],
         [7]]])
tensor([[[1, 2, 3, 4],
         [5, 6, 7, 8],
         [5, 6, 7, 8]],

        [[4, 5, 6, 7],
         [5, 6, 7, 8],
         [5, 6, 7, 8]]])
tensor([[1],
        [4]])
```

- [交并比手写公式推导（IoU）](https://blog.csdn.net/weixin_52366977/article/details/127109929?spm=1001.2014.3001.5506)

- torch.full()

```python
torch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor
```

**返回创建size大小的维度，里面元素全部填充为fill_value**

```python
x = torch.full(size=(2,3),fill_value=5)
x
输出结果如下：
tensor([[5, 5, 5],
        [5, 5, 5]])
```

- torch.nonzero()

1.输入是一维张量，返回一个包含输入 input 中非零元素索引的张量，输出张量中的每行包含 input 中非零元素的索引，输出是二维张量torch.size(z,1)， z 是输入张量 input 中所有非零元素的个数。

2.输入是n维张量，如果输入 input 有 n 维,则输出的索引张量的size为torch.size(z,n) , 这里 z 是输入张量 input 中所有非零元素的个数。
**无论输入是几维，输出张量都是两维，每行代表输入张量中非零元素的索引位置(在所有维度上面的位置)。**
**返回 input中非零元素的索引下标，n维input 中的元素的索引有n 个维度的索引下标。**

```python
x = torch.tensor([1, 1, 1, 0, 1])
y = torch.nonzero(x)
x,x.shape,y,y.shape
输出结果如下：
(tensor([1, 1, 1, 0, 1]),
 torch.Size([5]),
 tensor([[0],
         [1],
         [2],
         [4]]),
 torch.Size([4, 1]))


x = torch.tensor([[0.6, 0.0, 0.9, 0.0],
                  [0.0, 0.4, 0.0, 0.0],                            
                  [0.0, 0.0, 1.2, 0.0],
                  [0.0, 0.7, 0.0,-0.4]])
y = torch.nonzero(x)
x,x.shape,y,y.shape
输出结果如下：
(tensor([[ 0.6000,  0.0000,  0.9000,  0.0000],
         [ 0.0000,  0.4000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  1.2000,  0.0000],
         [ 0.0000,  0.7000,  0.0000, -0.4000]]),
 torch.Size([4, 4]),
 tensor([[0, 0],
         [0, 2],
         [1, 1],
         [2, 2],
         [3, 1],
         [3, 3]]),
 torch.Size([6, 2]))


x = torch.randn(2,3,4)
y = (x>0.1)
z = torch.nonzero(y)
x,x.shape,y,y.shape,z,z.shape
输出结果如下：
(tensor([[[ 0.3326, -0.9972, -0.4871, -1.3885],
          [ 0.4679, -1.7913,  2.0604,  0.3150],
          [-0.6156, -0.5204,  0.2902, -0.0780]],
 
         [[ 1.2206, -0.7150, -0.1662,  0.5120],
          [ 0.2907,  0.1285,  0.8520, -1.2698],
          [ 0.5176, -0.3800,  0.4408,  0.5073]]]),
 torch.Size([2, 3, 4]),
 tensor([[[ True, False, False, False],
          [ True, False,  True,  True],
          [False, False,  True, False]],
 
         [[ True, False, False,  True],
          [ True,  True,  True, False],
          [ True, False,  True,  True]]]),
 torch.Size([2, 3, 4]),
 tensor([[0, 0, 0],
         [0, 1, 0],
         [0, 1, 2],
         [0, 1, 3],
         [0, 2, 2],
         [1, 0, 0],
         [1, 0, 3],
         [1, 1, 0],
         [1, 1, 1],
         [1, 1, 2],
         [1, 2, 0],
         [1, 2, 2],
         [1, 2, 3]]),
 torch.Size([13, 3]))
```

- torch.long()

取整数部分

```python
import torch
x = torch.randn(3,4)
x,x.long()
=>(tensor([[-0.09, -0.55,  1.13,  0.04],
         [ 1.19,  2.01,  0.91,  1.59],
         [-1.11,  0.27, -0.69,  2.03]]),
 tensor([[ 0,  0,  1,  0],
         [ 1,  2,  0,  1],
         [-1,  0,  0,  2]]))
```

- view()，torch.flatten(),torch.nn.Flatten() [here](https://blog.csdn.net/hjkdh/article/details/123082852?ops_request_misc=&request_id=&biz_id=102&utm_term=torch.flatten&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-9-123082852.nonecase&spm=1018.2226.3001.4187)

view():**其实就是把原先tensor中的数据进行排列，排成一行，然后根据所给的view()中的参数从一行中按顺序选择组成最终的tensor**。

torch.nn.Flatten():

**torch.nn.Flatten(start_dim=1,end_dim=-1)**

start_dim与end_dim代表合并的维度，开始的默认值为1，结束的默认值为-1

torch.flatten():

**torch.flatten(t, start_dim=0, end_dim=-1)**

torch.flatten默认是从0开始的

- model.train() 和 model.eval() 原理与用法 [here](https://blog.csdn.net/weixin_44211968/article/details/123774649?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168602136216800186558673%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=168602136216800186558673&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-4-123774649-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=model.trian%28%29%20%20dropout&spm=1018.2226.3001.4187)

一般用法是：在训练开始之前写上 model.trian() ，在测试时写上 model.eval() model.train()，作用是 **启用 batch normalization 和 dropout**

model.eval()的作用是 **不启用 Batch Normalization 和 Dropout**

- nn.Module的**children()**与**modules()**方法、如何获取网络的某些层 [here](https://blog.csdn.net/pengchengliu/article/details/113878358?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168654439416800197046767%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=168654439416800197046767&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-113878358-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=nn.children%20nn.modules&spm=1018.2226.3001.4187)

children()与modules()都是返回网络模型里的组成元素，但是children()返回的是最外层的元素，modules()返回的是所有的元素，包括不同级别的子元素。

- 模型权重初始化

```python
#1
def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
self._initialize_weights()                
#2   
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
```



- nn.moduleList 和Sequential用法以及区别  [here](https://blog.csdn.net/e01528/article/details/84397174?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168654579716800211593221%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168654579716800211593221&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-84397174-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=nn.modellist&spm=1018.2226.3001.4187)

nn.Sequential()模型建立方式

第一种写法：
nn.Sequential()对象.add_module(层名，层class的实例）

```python
net1 = nn.Sequential()
net1.add_module('conv', nn.Conv2d(3, 3, 3))
net1.add_module('batchnorm', nn.BatchNorm2d(3))
net1.add_module('activation_layer', nn.ReLU())
```

 第二种写法：
nn.Sequential(*多个层class的实例)

```python
net2 = nn.Sequential(
     		nn.Conv2d(3, 3, 3),
     		nn.BatchNorm2d(3),
     		nn.ReLU()
    )
```

第三种写法：
nn.Sequential(OrderedDict([*多个(层名，层class的实例)]))

```python
from collections import OrderedDict

net3= nn.Sequential(OrderedDict([
      		('conv', nn.Conv2d(3, 3, 3)),
      		('batchnorm', nn.BatchNorm2d(3)),
      		('activation_layer', nn.ReLU())
    ]))
```

nn.modulelist()模型建立方式

```python
class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])
 
    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed         using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x
```

**区别**：

不同点1：

nn.Sequential内部实现了forward函数，因此可以不用写forward函数。而nn.ModuleList则没有实现内部forward函数。

不同点2：

nn.Sequential可以使用OrderedDict对每层进行命名

不同点3：

nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。而nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言。

不同点4：

有的时候网络中有很多相似或者重复的层，我们一般会考虑用 for 循环来创建它们，而不是一行一行地写

- **PyTorch 中，nn 与 nn.functional** 的区别 [here](https://www.zhihu.com/question/66782101/answer/579393790)

两者的相同之处：

- `nn.Xxx`和`nn.functional.xxx`的实际功能是相同的，即`nn.Conv2d`和`nn.functional.conv2d` 都是进行卷积，`nn.Dropout` 和`nn.functional.dropout`都是进行dropout，。。。。。； 
- 运行效率也是近乎相同。

`nn.functional.xxx`是函数接口，而`nn.Xxx`是`nn.functional.xxx`的类封装，并且**`nn.Xxx`都继承于一个共同祖先`nn.Module`。**这一点导致`nn.Xxx`除了具有`nn.functional.xxx`功能之外，内部附带了`nn.Module`相关的属性和方法，例如`train(), eval(),load_state_dict, state_dict `等。

两者的差别之处：

- **两者的调用方式不同。**

`nn.Xxx` 需要先实例化并传入参数，然后以[函数调用](https://www.zhihu.com/search?q=函数调用&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A579393790})的方式调用实例化的对象并传入输入数据。

```python
inputs = torch.rand(64, 3, 244, 244)
conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)
out = conv(inputs)
```

`nn.functional.xxx`同时传入输入数据和weight, bias等其他参数 。

```python
weight = torch.rand(64,3,3,3)
bias = torch.rand(64) 
out = nn.functional.conv2d(inputs, weight, bias, padding=1)
```

- **`nn.Xxx`继承于`nn.Module`， 能够很好的与`nn.Sequential`结合使用， 而`nn.functional.xxx`无法与`nn.Sequential`结合使用。**

```python
 fm_layer = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(num_features=64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Dropout(0.2)
  )
```

- **`nn.Xxx`不需要你自己定义和管理weight；而`nn.functional.xxx`需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。**

使用`nn.Xxx`定义一个CNN 。

```python
class CNN(nn.Module):
    
    
    def __init__(self):
        super(CNN, self).__init__()
        
        self.cnn1 = nn.Conv2d(in_channels=1,  out_channels=16, kernel_size=5,padding=0)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        
        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5,  padding=0)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)
        
        self.linear1 = nn.Linear(4 * 4 * 32, 10)
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        out = self.maxpool1(self.relu1(self.cnn1(x)))
        out = self.maxpool2(self.relu2(self.cnn2(out)))
        out = self.linear1(out.view(x.size(0), -1))
        return out
```

使用`nn.function.xxx`定义一个与上面相同的CNN。

```python
class CNN(nn.Module):
    
    
    def __init__(self):
        super(CNN, self).__init__()
        
        self.cnn1_weight = nn.Parameter(torch.rand(16, 1, 5, 5))
        self.bias1_weight = nn.Parameter(torch.rand(16))
        
        self.cnn2_weight = nn.Parameter(torch.rand(32, 16, 5, 5))
        self.bias2_weight = nn.Parameter(torch.rand(32))
        
        self.linear1_weight = nn.Parameter(torch.rand(4 * 4 * 32, 10))
        self.bias3_weight = nn.Parameter(torch.rand(10))
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        out = F.conv2d(x, self.cnn1_weight, self.bias1_weight)
        out = F.relu(out)
        out = F.max_pool2d(out)
        
        out = F.conv2d(x, self.cnn2_weight, self.bias2_weight)
        out = F.relu(out)
        out = F.max_pool2d(out)
        
        out = F.linear(x, self.linear1_weight, self.bias3_weight)
        return out
```

上面两种定义方式得到CNN功能都是相同的，至于喜欢哪一种方式，是个人口味问题，但PyTorch官方推荐：具有学习参数的（例如，conv2d, linear, batch_norm)采用`nn.Xxx`方式，没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用`nn.functional.xxx`或者`nn.Xxx`方式。但关于dropout，个人强烈推荐使用`nn.Xxx`方式，因为一般情况下只有训练阶段才进行dropout，在eval阶段都不会进行dropout。使用`nn.Xxx`方式定义dropout，在调用`model.eval()`之后，model中所有的dropout layer都关闭，但以`nn.function.dropout`方式定义dropout，在调用`model.eval()`之后并不能关闭dropout。

```python
class Model1(nn.Module):
    
    def __init__(self):
        super(Model1, self).__init__()
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        return self.dropout(x)
    
    
class Model2(nn.Module):

    def __init__(self):
        super(Model2, self).__init__()

    def forward(self, x):
        return F.dropout(x)
    

m1 = Model1()
m2 = Model2()
inputs = torch.rand(10)
print(m1(inputs))
print(m2(inputs))
print(20 * '-' + "eval model:" + 20 * '-' + '\r\n')
m1.eval()
m2.eval()
print(m1(inputs))
print(m2(inputs))
```

输出：

![img](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306121954391.webp)

从上面输出可以看出`m2`调用了`eval`之后，dropout照样还在正常工作。当然如果你有强烈愿望坚持使用`nn.functional.dropout`，也可以采用下面方式来补救。

```python
 class Model3(nn.Module):

    def __init__(self):
        super(Model3, self).__init__()

    def forward(self, x):
        return F.dropout(x, training=self.training)
    
 net.train()=>self.training=True
 net.eval(=>self.training=False
```

------

什么时候使用`nn.functional.xxx`，什么时候使用`nn.Xxx`?

> 这个问题依赖于你要解决你问题的复杂度和个人风格喜好。在`nn.Xxx`不能满足你的功能需求时，`nn.functional.xxx`是更佳的选择，因为`nn.functional.xxx`更加的灵活(更加接近底层），你可以在其基础上定义出自己想要的功能。 

个人偏向于在能使用`nn.Xxx`情况下尽量使用，不行再换`nn.functional.xxx` ，感觉这样更能显示出网络的层次关系，也更加的纯粹（所有layer和model本身都是Module，一种和谐统一的感觉）。

- 保存模型参数 [here](https://blog.csdn.net/weixin_40522801/article/details/106563354?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168655147316800225542787%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=168655147316800225542787&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-4-106563354-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=model.load_state_dict&spm=1018.2226.3001.4187)

torch.save(net.state_dict(), save_path)

加载模型参数

model.load_state_dict(torch.load(weights_path))

- Batch Normalization详解以及pytorch实验 [here](https://blog.csdn.net/qq_37541097/article/details/104434557)

（1）训练时要将traning参数设置为True，在验证时将trainning参数设置为False。在pytorch中可通过创建模型的model.train()和model.eval()方法控制。

（2）batch size尽可能设置大点，设置小后表现可能很糟糕，设置的越大求的均值和方差越接近整个训练集的均值和方差。

（3）建议将bn层放在卷积层（Conv）和激活层（例如Relu）之间，且卷积层不要使用偏置bias，因为没有用，参考下图推理，即使使用了偏置bias求出的结果也是一样的

- 批量读取文件、图片

```python
import os
 # load image
# 指向需要遍历预测的图像文件夹
imgs_root = "/home/czh/git/deep-learning-for-image-processing/data_set/flower_data/val/tulips"
assert os.path.exists(imgs_root), f"file: '{imgs_root}' dose not exist."
# 读取指定文件夹下所有jpg图像路径
img_path_list = [os.path.join(imgs_root, i) for i in os.listdir(imgs_root) if i.endswith(".jpg")]
#print(img_path_list)
os.listdir(imgs_root)
```

Python遍历目录下的文件（os.walk 、os.listdir的用法） [here](https://blog.csdn.net/weixin_42272869/article/details/123160931?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168662678316800215030376%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168662678316800215030376&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-123160931-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=os.listdir&spm=1018.2226.3001.4187)

python的endswith()的用法及实例 [here](https://blog.csdn.net/weixin_50853979/article/details/125817413?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168662756516800213097993%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168662756516800213097993&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-125817413-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=.endswith&spm=1018.2226.3001.4187)

# 其他

- json.dumps() 与 json.loads() 用法 [见此](https://blog.csdn.net/qq_45859826/article/details/124158012?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168601495916800184145288%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168601495916800184145288&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-124158012-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=json.dumps&spm=1018.2226.3001.4187)   

  [文件操作、对.txt文本文件的操作（读、写、修改、复制、合并）、对json文本文件的操作、json字符串与字典的相互转换](https://blog.csdn.net/y_three/article/details/128162265?ops_request_misc=&request_id=&biz_id=102&utm_term=json%E6%A0%BC%E5%BC%8F%E5%92%8Ctxt%E7%9A%84%E5%8C%BA%E5%88%AB&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-128162265.142^v88^control,239^v2^insert_chatgpt&spm=1018.2226.3001.4187)

json.load：表示读取文件，返回**python对象**
json.dump：表示写入文件，文件为**json字符串**格式，无返回
json.dumps：将python中的字典类型转换为字符串类型，返回json字符串 **[dict→str]**
json.loads：将json字符串转换为字典类型，返回python对象 **[str→dict]**
load和dump处理的主要是 文件
loads和dumps处理的是 字符串

```python
 #train
    # {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}
    flower_list = train_dataset.class_to_idx
    cla_dict = dict((val, key) for key, val in flower_list.items())
    # write dict into json file
    json_str = json.dumps(cla_dict, indent=4)
    with open('class_indices.json', 'w') as json_file:
        json_file.write(json_str)
        
        
#predict
json_path = 'class_indices.json'
assert os.path.exists(json_path), "file: '{}' dose not exist.".format(json_path)

with open(json_path, "r") as f:
    class_indict = json.load(f)
```

 **json.dumps()**

```python
import json

data = {
    'fruit':'apple',
    'vegetable':'cabbage'
}
print(data,type(data))

data = json.dumps(data)  # dict转json
print(data,type(data))

#result
{'fruit': 'apple', 'vegetable': 'cabbage'} <class 'dict'>
{"fruit": "apple", "vegetable": "cabbage"} <class 'str'>
```

**json.loads()**

```python
data = """{
"fruit": "apple",
"vegetable": "cabbage"
}"""
# 一般此时data为request.text返回值
print(data, type(data))
data = json.loads(data)
print(data, type(data))
#result
{
"fruit": "apple",
"vegetable": "cabbage"
} <class 'str'>
{'fruit': 'apple', 'vegetable': 'cabbage'} <class 'dict'>
```

**json.dump()**

```python
写str
data = "wyt"
with open('b.json', 'w') as f:
    json.dump(data, f)

with open('b.json','r',encoding='utf-8') as f :
    f_str = json.load(f)
    print(f_str,type(f_str))
#
wyt <class 'str'>



写dict
data = {
    'fruit':'apple',
    'vegetable':'cabbage'
}
with open('b.json', 'w') as f:
    json.dump(data, f)

with open('b.json','r',encoding='utf-8') as f :
    f_str = json.load(f)
    print(f_str,type(f_str))
#
{'fruit': 'apple', 'vegetable': 'cabbage'} <class 'dict'>
```

**json.load()**

a.json中存在：

```python
{
"fruit": "apple",
"vegetable": "cabbage"
}
```

a.py中：

```python
with open('a.json','r',encoding='utf-8') as f :
    f_str = json.load(f)
    print(f_str,type(f_str))
    
#
{'fruit': 'apple', 'vegetable': 'cabbage'} <class 'dict'>
```

- tqdm           [1](https://blog.csdn.net/weixin_44878336/article/details/124894210?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168601815016800185859765%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168601815016800185859765&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-5-124894210-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=tqdm&spm=1018.2226.3001.4187)         [2](https://blog.csdn.net/wxd1233/article/details/118371404?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168601815016800185859765%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168601815016800185859765&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-118371404-null-null.142^v88^control,239^v2^insert_chatgpt&utm_term=tqdm&spm=1018.2226.3001.4187)

tqdm() 的源码为：

```python
def __init__(self, iterable=None, desc=None, total=None, leave=True, file=None,
                 ncols=None, mininterval=0.1, maxinterval=10.0, miniters=None,
                 ascii=None, disable=False, unit='it', unit_scale=False,
                 dynamic_ncols=False, smoothing=0.3, bar_format=None, initial=0,
                 position=None, postfix=None, unit_divisor=1000, write_bytes=None,
                 lock_args=None, nrows=None, colour=None, delay=0, gui=False,
                 **kwargs):
```

其中各个参数的含义为：

```python
iterable: 可迭代的对象, 在手动更新时不需要进行设置
desc: 字符串, 进度条左边的描述文字
total: 总的项目数
leave: bool值, 迭代完成后是否保留进度条
file: 输出指向位置, 默认是终端, 一般不需要设置
ncols: 调整进度条宽度, 默认是根据环境自动调节长度, 如果设置为0, 就没有进度条, 只有输出的信息
unit: 描述处理项目的文字, 默认是'it', 例如: 100 it/s, 处理照片的话设置为'img' ,则为 100 img/s
unit_scale: 自动根据国际标准进行项目处理速度单位的换算, 例如 100000 it/s >> 100k it/s
```

train_bar.desc()等价于train_bar.set_description()

```python
from tqdm import tqdm

for epoch in range(num_epochs):
    losses = []
    accuracy = []
    # for data,targets in tqdm(train_loadr,leave=False) # 进度显示在一行
    loop = tqdm((train_loader), total = len(train_loader))
    for data,targets in loop:
        # Get data to cuda if possible
        data = data.to(device=device)
        targets = targets.to(device=device)

        # forward
        scores = model(data)
        loss = criterion(scores,targets)
        losses.append(loss)
        # backward
        optimizer.zero_grad()
        loss.backward()
        _,predictions = scores.max(1)
        num_correct = (predictions == targets).sum()
        running_train_acc = float(num_correct) / float(data.shape[0])
        accuracy.append(running_train_acc)
        # gardient descent or adam step
        optimizer.step()
        loop.set_description(f'Epoch [{epoch}/{num_epochs}]')
        loop.set_postfix(loss = loss.item(),acc = running_train_acc)
```

