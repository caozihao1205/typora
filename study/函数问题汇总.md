# python函数-问题

- ## [yeild](https://www.cnblogs.com/f-ck-need-u/p/10266226.html#!comments)


```python
for x in [s * 2 for s in "abcde"]: print(x, end=" ") 

for x in map( (lambda s: s * 2), "abcde" ): print(x, end=" ")
```

- [map](https://www.cnblogs.com/superxuezhazha/p/5714970.html)

- [lambda](https://www.cnblogs.com/evening/archive/2012/03/29/2423554.html)

```python
a=[1.0,2.0]
args=(1.0,2.0)
[a+float(b) for a,b in zip(a,args)]

#[2.0, 4.0]
```

```python
[[] for _ in range(5)]
#[[], [], [], [], []]
```

```python
(1,2)+(5,)
#(1, 2, 5)
#元组中只包含一个元素时，需要在元素后面添加逗号来消除歧义(5,)
#元组相加相当于列表的append
```

元组和字符串一样是不可变对象。不能增删改。

创建：a=()/tuple()

访问：

```python
tup1 = ('physics', 'chemistry', 1997, 2000);
tup2 = (1, 2, 3, 4, 5, 6, 7 );
 
print "tup1[0]: ", tup1[0]
print "tup2[1:5]: ", tup2[1:5]
#以上实例输出结果：
#tup1[0]:  physics
#tup2[1:5]:  [2, 3, 4, 5]
```

修改

```python
tup1 = (12, 34.56);
tup2 = ('abc', 'xyz');
 
# 以下修改元组元素操作是非法的。
# tup1[0] = 100;
 
# 创建一个新的元组
tup3 = tup1 + tup2;
print tup3;
#以上实例输出结果：
#(12, 34.56, 'abc', 'xyz')
```

- np.power:power(x, y) 函数，计算 x 的 y 次方。

- math.gamma(n):(n-1)!

eg.math.gamma(3+1)=>6

- ```python
  os.makedirs(cache_dir, exist_ok=True)
  #创建一个新的目录(cache_dir), 如果这个目录不存在的话。exist_ok=True表示如果目录已经存在，则不会发生任何错误。否则，如果exist_ok=False，如果目录已经存在，会抛出一个OSError异常。
  
  ```

- os.path.splitext()与os.path.split()

  ```python
  #os.path.splitext() 将文件名和扩展名分开
  
  #os.path.split() 返回文件的路径和文件名
  import os
   
  #os.path.join() 将分离的部分合成一个整体
  filename=os.path.join('/home/ubuntu/python_coding','split_func')
  print filename
  #输出为：/home/ubuntu/python_coding/split_func
   
  #os.path.splitext()将文件名和扩展名分开
  fname,fename=os.path.splitext('/home/ubuntu/python_coding/split_func/split_function.py')
  print 'fname is:',fname
  print 'fename is:',fename
  #输出为：
  # fname is:/home/ubuntu/python_coding/split_func/split_function
  #fename is:.py
   
  #os.path.split（）返回文件的路径和文件名
  dirname,filename=os.path.split('/home/ubuntu/python_coding/split_func/split_function.py')
  print dirname
  print filename
  #输出为：
  # /home/ubuntu/python_coding/split_func
  #split_function.py
  ```

- os.path.basename():接收一个文件路径作为参数，并返回该文件路径中的最后一个目录或文件的名称。

  ```python
  os.path.basename('/usr/local/bin/python3.7')
  
  # 'python3.7'
  ```

  ```python
  def download_extract(name, folder=None):  #@save
      """下载并解压zip/tar文件"""
      fname = download(name)
      base_dir = os.path.dirname(fname)
      data_dir, ext = os.path.splitext(fname)
      if ext == '.zip':
          fp = zipfile.ZipFile(fname, 'r')
      elif ext in ('.tar', '.gz'):
          fp = tarfile.open(fname, 'r')
      else:
          assert False, '只有zip/tar文件可以被解压缩'
      fp.extractall(base_dir)
      return os.path.join(base_dir, folder) if folder else data_dir
  
  ```

- [hashlib模块](https://blog.csdn.net/qdPython/article/details/123826017?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168008243016800225578529%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168008243016800225578529&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-123826017-null-null.142^v77^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=hashlib&spm=1018.2226.3001.4187)

  ```python
  #简单的加密方式
  import hashlib
  s = 'abc'
  md5 = hashlib.md5()    # 选择加密方式，初始化一个加密
  md5.update(s.encode('utf-8'))    # 将要加密的内容，添加到m中
  print(md5.hexdigest())
  
  ```

  

- pd.iloc[:,1,-1]切片的时候包括头不包括尾部。

  ![image-20230329180640577](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520373.png)

- [pd.concat()](https://blog.csdn.net/Mr_HHH/article/details/79488445?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168007316016800184149403%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168007316016800184149403&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79488445-null-null.142^v77^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%20pd.concat&spm=1018.2226.3001.4187):可以将数据根据不同的轴作简单的融合

  ```python
  pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,keys=None, levels=None, names=None, verify_integrity=False)
  #objs: series，dataframe或者是panel构成的序列lsit 
  frames = [df1, df2, df3]
  result = pd.concat(frames)
  
  pd.concat([df1, df4], axis=1)
  ```

  - 1

  当axis = 0的时候，concat就是列对齐，然后将不同行名称的两张表合并

  ![image-20230329183202657](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520370.png)

  

  当axis = 1的时候，concat就是行对齐，然后将不同列名称的两张表合并

  ![image-20230329183233188](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520379.png)

- [dataframe[]](https://note.youdao.com/web/#/file/recent/note/WEB1823af4e69fc01d71915da3b706b04dc/):

  ```python
  print(df[:20])  #前20行 
  print(df["Row_Labels"])#取Row_Labels这一列 
  # pandas取行或者列的注意点： 
  1.方括号中写数字，表示取行，对行进行操作 
  2.方括号中写字符串，表示取列的索引，对列进行操作。
  ```

- df.dtypes:查看数据类型,输出结果是series

- [pd.get_dummies()](https://blog.csdn.net/lindaicoding/article/details/118692358)

  ```python
  data = pd.DataFrame({"学号":[1,2,3,4],
                      "录取":["清华","北大","清华","蓝翔"],
                      "学历":["本科","本科","本科","专科"]})
  pd.get_dummies(data)
  ```

  ![img](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520433.png)

- slice():主要用在切片操作函数里的参数传递

  `class slice(start, stop[, step])`
  参数：start起始，stop终止，step步长

  ```python
  indices = slice(0, 5, 2)
  alst = [1,2,3,4,5,6,7,8,9]
  alst[indices], alst[0:5:2]
  #([1, 3, 5], [1, 3, 5])
  ```

- f的用法

  ```python
  print(f'{float(4.000)}')
  #4.0
  print(f'{float(4.000):f}')
  #4.000000默认六位小数
  print(f'{float(4.000):0.5f}')
  #4.00000
  ```

- **hasattr(object, name)**

  ```python
  object -- 对象。
  name -- 字符串，属性名。
  return
  如果对象有该属性返回 True，否则返回 False。
  
  class variable:
      x = 1
      y = 'a'
      z = True
  dd = variable() 
  print(hasattr(dd, 'x'))
  print(hasattr(dd, 'y'))
  print(hasattr(dd, 'z'))
  print(hasattr(dd, 'no'))
  结果：
  True
  True
  True
  False
  
  if not hasattr(y, "__len__"):
     #判断y是否是可迭代对象
  ```

- **isinstance(object,classinfo)**:

  用来判断一个函数是否是一个已知的类型，类似 type()

  ```python
  a = 2
  isinstance(a,int) # 结果返回 True
  ```

- **ndim**:返回的是**数组**(列表没有维度)的维度，返回的只有一个数，该数即表示数组的维度.

  ```python
  data1=[1,2,3,4]
  data2=[[1,2],[3,4]]
  a=np.array(data1)
  b=np.array(data2)
  print(a.ndim,b.ndim)
  ```

- 一张图像上画多个子图

使用plt.figure(arg)创建画板，arg为画板名称

使用plt.subplot(arg1, arg2, arg3)方法创建画纸，并选择当前画纸并绘图。其中，ag1代表第几行，arg2代表第几列，arg3代表第几个图像

1. `plt.figure()`

​      `plt.subplot(121)`

​	  `plt.plot`

2. `_, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)`

   `axes[0].plot`

   

```python
x = np.arange(0, 100, 10)

plt.figure(1)  # 生成第一个图，且当前要处理的图为fig.1

plt.subplot(1, 2, 1)  # fig.1是一个一行两列布局的图，且现在画的是左图
y1 = np.exp(x)
plt.plot(x, y1, color="r", linestyle="-", marker="^", linewidth=1)  # 画图
plt.xlabel("x")
plt.ylabel("y1")

plt.figure(1)  # 当前要处理的图为fig.1，而且当前图是fig.1的左图
plt.subplot(1, 2, 2)  # 当前图变为fig.1的右图
y2 = np.exp(1.5 * x)
plt.plot(x, y2, color="b", linestyle="-", marker="v", linewidth=1)
plt.xlabel("x")
plt.ylabel("y2")

plt.show()
```

- [OrderedDict](https://blog.csdn.net/longshaonihaoa/article/details/108469859?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168016060616800213090270%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168016060616800213090270&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-108469859-null-null.142^v77^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=OrderedDict&spm=1018.2226.3001.4187): 按照有序插入顺序存储 的有序字典

- *(1,2,3)=>1,2,3

- python中*vars代表解包元组，**vars代表解包字典，通过这种语法可以传递不定参数

- **rstrip()** 的用法 以及和 strip() 的区别

  strip() 方法用于移除字符串**头尾**指定的字符（默认为空格或换行符）或字符序列。
  rstrip() 删除 string 字符串**末尾**的指定字符（默认为空格）

- [readlines(),read()和readline()](https://blog.csdn.net/digitalkee/article/details/113620458)

  1.read()

  这个函数是把一个文档当成一个字符串（结尾有空字符串）返回。也就是一个文档存储在一个字符串对象中，如果文件很大，就很耗内存。read()和原来的文档相比，输出的时候结尾多了一个'\n'（换行）

  2.readlines()，是把一个文档的每一行（包含行前的空格，行末加一个\n），作为列表的一个元素，存储在一个list中。每一个行作为list的一个元素。readlines()和原文档相比，每行之后都多出一个空行，因为原来每行末尾都加了'\n'，要想和原文档输出一样，那么每个行末尾加上rstrip()即可消除右边空格，左边空格不变。

  3.readline()，类似于readlines()，不过它每次只读取文档的一行。以后需要逐步循环（一般用whicl循环）,最后遍历整个文件。输出情况同readlines()

- [collection](https://www.cnblogs.com/songqingbo/p/5137785.html)

  ```python
  collections.Counter(labels.values()).most_common()[-1][1]
  
  #collections.Counter(labels.values()) : 这个部分使用 collections 模块中的 Counter() 函数，对 labels.values() 中的元素进行计数。
  #.most_common() : 这个方法返回计数器中出现次数最多的元素和对应的次数，按照出现次数从大到小排序。
  #[-1] : 这个索引选取最常见元素的元组，即出现次数最少的元素和对应的次数。
  #[1] : 这个索引选取元组中的次数，即出现次数最少的元素的数量。
  ```

  ```python
  import collections
  
  obj = collections.Counter('sjndsjkdsdmslaladsldsldms')
  
  print("输出字符出现的次数字典:")
  for k,v in obj.items():
      print("{%s:%s}" % (k,v))
  
  print("输出每一个字符:") #遍历获取原始字符元素
  for k in obj.elements():
      print(k)
  
  print("输出前四个出现次数最多的字符:")
  for k in obj.most_common(4):
      print(k)
  
  输出结果：
  	输出字符出现的次数字典:
  	{s:7}
  	{l:4}
  	{m:2}
  	{d:6}
  	{k:1}
  	{n:1}
  	{j:2}
  	{a:2}
  	输出每一个字符:
  	s
  	s
  	s
  	s
  	s
  	s
  	s
  	l
  	l
  	l
  	l
  	m
  	m
  	d
  	d
  	d
  	d
  	d
  	d
  	k
  	n
  	j
  	j
  	a
  	a
  	输出前四个出现次数最多的字符:
  	('s', 7)
  	('d', 6)
  	('l', 4)
  	('m', 2)
  ```

- [dict get函数](https://www.cnblogs.com/cgmcoding/p/14428632.html)

  get() 函数返回指定键key的值value

  返回指定键的值，**如果键不在字典中返回默认值 None 或者设置的默认值。**

  ```python
  dict.get(key, default=None)
  
  #key -- 字典中要查找的键。
  #default -- 如果指定键的值不存在时，返回该默认值。
  ```

- [list extend()](https://blog.csdn.net/mz02230909mz/article/details/115704852)

  扩充列表元素内容的方法，在一定程度上其行为有点像append。只是在接受的参数以及最终的效果上有些差异。

  ```python
  list1 = [1,2,4]
  list2 = [12,6]
  list1.extend(list2)
  print(list1)
  #结果
  [1, 2, 4, 12, 6]
  
  list1 = [1,2,4]
  list2 =[[12,6],123]
  list1.extend(list2)
  print(list1)
  #结果
  [1, 2, 4, [12, 6],123]
  
  list1 = [1,2,4]
  list2 =[[12,6],123]
  list1.append(list2)
  print(list1)
  #结果
  [1, 2, 4, [[12,6], 123]]
  ```

  Extend是把每个元素都作为一个独立的元素扩充到原来的列表，而append则是把整个扩充列表作为一个元素追加到列表最后。

- ```python
  from PIL import Image
  from matplotlib import pyplot as plt
  img=plt.imread(r'C:\Users\小黑\Pictures\catdog.jpg')
  #等价于
  img=Image.open(r'C:\Users\小黑\Pictures\catdog.jpg')
  ```

  plt.imread等价于Image.open

- plt.Rectangle()

  ```python
  d2l.plt.Rectangle(xy=(400,30),width=100,height=150,fill=True,edgecolor='green',linewidth=2)
  ```

  这个函数的作用是画矩形框，通过坐标与宽高。

  这个函数的作用是画矩形框，通过坐标与宽高。
  xy=(400,30)：左上角的坐标，有了左上角坐标，然后就可以开始画了，横向往右画宽度，纵向向下画高度
  fill=True或False：表示是否填充
  edgecolor='green'：边框的颜色
  linewidth=2：画笔的大小

- fig.axes.add_patch

  fig.axes.add_patch(d2l.plt.Rectangle(xy=(20,50),width=100,height=150,fill=True,edgecolor='green',linewidth=2))

  将上面画好的矩形框，添加到画布上面，从字面意思来看，就相当于是在画布中加一块补丁

- [set_index（）](https://blog.csdn.net/Ajdidfj/article/details/123178391)

  DataFrame.set_index(*keys*, *drop=True*, *append=False*, *inplace=False*, *verify_integrity=False*)

  使用现有列设置 DataFrame 索引。

- df.iterrows()和df.items()

  df变成series

```python
import pandas as pd
df=pd.DataFrame({'A':[1,2],'B':[3,4]})
print('===df===')
print(df)
print('===df.iterrows()===')
for index,row in df.iterrows():
    print('行号：',index)
    print('该行的每列名称和值:')
    print(row)
print('===df.items()===')
for index1,column in df.items():
    print('列号:',index1)
    print('该列的每行名称和值：')
    print(column)
    
#结果：
===df===
   A  B
0  1  3
1  2  4
===df.iterrows()===
行号： 0
该行的每列名称和值:
A    1
B    3
Name: 0, dtype: int64
行号： 1
该行的每列名称和值:
A    2
B    4
Name: 1, dtype: int64
===df.items()===
列号: A
该列的每行名称和值：
0    1
1    2
Name: A, dtype: int64
列号: B
该列的每行名称和值：
0    3
1    4
Name: B, dtype: int64
```

- np.expand_dims(np,dim=X)

  用于扩展数组的形状

```python
#一维
np.expand_dims((1,2,3,4),1)
=>array([[1],
       [2],
       [3],
       [4]])
#二维
np.expand_dims([[1,2,3,4],[5,6,7,8]],0)
=>array([[[1, 2, 3, 4],
        [5, 6, 7, 8]]])

np.expand_dims([[1,2,3,4],[5,6,7,8]],1)
=>array([[[1, 2, 3, 4]],

       [[5, 6, 7, 8]]])
np.expand_dims([[1,2,3,4],[5,6,7,8]],2)
=>array([[[1],
        [2],
        [3],
        [4]],

       [[5],
        [6],
        [7],
        [8]]])
```

- [torch.permute()](https://blog.csdn.net/weixin_41377182/article/details/120808310)

交换数组的维度

- np.array().**reshape(-1)**

  torch.tensor().**reshape(-1)**

  自动把多维数组变成一维数组。

  ```python
  a=d2l.torch.tensor([[1,2,3],[4,5,6]])
  a.reshape(-1)
  =>tensor([1, 2, 3, 4, 5, 6])#变成一维
  a.reshape(1,6)
  =>tensor([[1, 2, 3, 4, 5, 6]])#二维，维度不变
  ```

- 

# pytorch

TensorDataset():把输入的两类数据进行一 一对应；

DataLoader：重新排序

torchvision数据集下载位置修改

1.torchvision.datasets:一些加载数据的函数及常用的数据集接口；
2.torchvision.models:包含常用的模型结构，例如AlexNet，VGG，ResNet等；
3.torchvision.transforms:常用的一些图片变换，例如图片裁剪、选择等；
4.torchvison.utils:其他一些有用的方法

```python
trans = transforms.ToTensor()
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)



#其中，
root：表示数据集下载保存位置
train：表示下载的数据集是不是训练集，True表示训练集，False表示测试集
download：表示数据集是否需要下载
transform：表示图片变换的一系列操作

root='/'：表示根目录下，如果你的代码保存在D盘，就是下载到D盘根目录下
root='./'：表示当前文件夹下
root=''：效果等同于'./'
root='./data'：表示在当前文件夹下的data（如果没有，则会新建一个）文件夹下保存数据集
root='data'：效果等同于'./data'
root='../':表示上一层目录
root='../../':表示上上一层目录
```

```python
trans = [transforms.ToTensor()]
#trans是包括transforms.ToTensor()的列表
trans.insert(0, transforms.Resize(resize))
trans = transforms.Compose(trans)
##transforms.Compose（[.. , .. , ..]）
```

```python
y=torch.Tensor([1.0,2.0])
y.numel()

a.numel()是一个 PyTorch 方法, 它用于计算元素数量，即张量（Tensor）中元素的总数。在这个代码中，变量 "y" 应该是一个张量，这个方法就是用来统计 y 张量中的元素个数的。
```

```python
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
    
    这三行代码的功能是进行断言检查，确保训练和测试的精度和损失符合要求。如果断言不成立，则会抛出AssertionError异常。
    
具体来说：
	assert train_loss < 0.5, train_loss：确保训练损失小于0.5，否则抛出异常，并打印出train_loss的值。
	assert train_acc <= 1 and train_acc > 0.7, train_acc：确保训练精度在0.7到1.0之间，否则抛出异常，并打印出train_acc的值。
	assert test_acc <= 1 and test_acc > 0.7, test_acc：确保测试精度在0.7到1.0之间，否则抛出异常，并打印出test_acc的值。
	这样做的目的是为了确保模型的训练和测试结果是合理的，不会出现特别低或特别高的情况。
```

```python
backward(retain_graph=True)

retain_graph==True:进行一次backward之后，各个节点的值会清除，这样进行第二次backward会报错，如果加上retain_graph==True后,可以再来一次backward。
```

- 训练步骤： 1.建立网络 2. 损失函数 3. 优化器（根据反向传播求得梯度 用优化器更具体都来更新参数） 4. 从训练集取出数据，进行训练，先梯度清0，算损失，反向传播，然后优化

- ```python
  weight_decay
   trainer = torch.optim.SGD([
          {"params":net[0].weight,'weight_decay': wd},
          {"params":net[0].bias}], lr=lr)
  
  #第一个元素{"params":net[0].weight,'weight_decay': wd}表示要优化的参数为网络net的第一个层的权重参数，同时还设置了L2正则化（weight decay）的参数为wd。第二个元素{"params":net[0].bias}表示要优化的参数为网络net的第一个层的偏置参数。
  ```

- torch.rand和torch.randn

  torch.rand均匀分布默认[0,1）

  torch.randn标准正态分布,，默认均值为0，方差为1

- torch.clamp():

  torch.clamp()等价于torch.clip()，用于设置 input tensor 在 min 和 max 之间。如果 input tensor 中元素的值小于 min，则设置该元素值为 min，如果 input tensor 中元素的值大于 max，则设置该元素值为 max。如果 min 大于 max，则将元素值设置为 max。如果 min 为None，则无下界，如果 max 为 None，则无上界。

```python
torch.clamp(input, min=None, max=None) → Tensor
Tensor.clamp(min=None, max=None) → Tensor
torch.clip(input, min=None, max=None) → Tensor
Tensor.clip(min=None, max=None) → Tensor
```

- torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)

  `params(iterable)：可用于迭代优化的参数或者定义参数组的dicts。`
  `lr (float, optional) ：学习率(默认: 1e-3)，更新梯度的时候使用`
  `betas (Tuple[float, float], optional)：用于计算梯度的平均和平方的系数(默认: (0.9, 0.999))`
  `eps (float, optional)：为了提高数值稳定性而添加到分母的一个项(默认: 1e-8)`
  `weight_decay (float, optional)：权重衰减(如L2惩罚)(默认: 0)，针对最后更新参数的时候，给损失函数中的加的一个惩罚参数，更新参数使用`

- **net.add_module(name, module)**

  name是添加模块的名称，module是要添加的子模块。在PyTorch中，每个模块都应该被命名，以便在不同的地方使用时能够区分。使用add_module方法添加的模块将会被添加到网络（net）中，并且可以通过名称进行访问。这使得网络更易于管理和调试。

- **m.weight.data *= m.weight.data.abs() >= 5**

  这里的运算符顺序是先判断大于等于得到布尔值再和左边相乘,因此这句话的意思是先判断m.weight.data是否大于5，得到F/T布尔值，对应0/1，再乘以m.weight.data。

- [torch.nn.Parameter](https://blog.csdn.net/weixin_44966641/article/details/118730730?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168016453916800182113998%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168016453916800182113998&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-118730730-null-null.142^v77^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=nn.parameter&spm=1018.2226.3001.4187)

  将一个不可训练的类型`Tensor`转换成可以训练的类型`parameter`并将这个`parameter`绑定到这个`module`里面(`net.parameter()`中就有这个绑定的`parameter`，所以在参数优化的时候可以进行优化

- **torch.matmul**可以用在更多维度的矩阵运算，

  **torch.mm**只能在两纬

- **model.zero_grad()和optimizer.zero_grad()**


```python
model.zero_grad()
optimizer.zero_grad()
```

首先，这两种方式都是把模型中参数的梯度设为0

当optimizer = optim.Optimizer(net.parameters())时，二者等效，其中Optimizer可以是Adam、SGD等优化器

```python
def zero_grad(self):
        """Sets gradients of all model parameters to zero."""
        for p in self.parameters():
            if p.grad is not None:
                p.grad.data.zero_()

```

- [**torch.stack**](https://blog.csdn.net/chensen99/article/details/124934391)

  是一个 PyTorch 中的函数，用于把多个张量（Tensor）按照给定的维度进行堆叠，返回一个新的张量。

  需要传入一个张量序列，以及一个整数 dim 或者axis参数。默认dim=0

  ```python
  a = torch.randn(2, 3)
  b = torch.randn(2, 3)
  
  c = torch.stack([a, b], dim=0)
  
  print(c.shape)
  
  #torch.Size([2, 2, 3])
  ```

  ```python、
  T1 = torch.tensor([[1, 2, 3],
          		[4, 5, 6],
          		[7, 8, 9]])
  T2 = torch.tensor([[10, 20, 30],
          		[40, 50, 60],
          		[70, 80, 90]])
  print(torch.stack((T1,T2),dim=0))#(2,3,3)，
  print(torch.stack((T1,T2),dim=1))
  print(torch.stack((T1,T2),dim=2))
  #
  tensor([[[ 1,  2,  3],
           [ 4,  5,  6],
           [ 7,  8,  9]],
  
          [[10, 20, 30],
           [40, 50, 60],
           [70, 80, 90]]])
  
  tensor([[[ 1,  2,  3],
           [10, 20, 30]],
  
          [[ 4,  5,  6],
           [40, 50, 60]],
  
          [[ 7,  8,  9],
           [70, 80, 90]]])
  
  tensor([[[ 1, 10],
           [ 2, 20],
           [ 3, 30]],
  
          [[ 4, 40],
           [ 5, 50],
           [ 6, 60]],
  
          [[ 7, 70],
           [ 8, 80],
           [ 9, 90]]])
  ```

- [torch.cat() 和 torch.stack()的区别](https://blog.csdn.net/hello_program_world/article/details/114340833?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168153904616800180659786%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168153904616800180659786&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-114340833-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=torch.cat%28%29%E5%92%8Ctorch.stack%28%29&spm=1018.2226.3001.4187)

1. torch.stack()是新增一维，属于增维操作（2*2，2*2 → 2*2*2）。

2. torch.cat()是在特定维度上进行拼接（2*2， 2*2 → 2*4）。

   ```python
   a0=torch.Tensor([[[[1,1,1,1],[2,2,2,2]]]])
   a1=torch.Tensor([[[[3,3,3,3],[4,4,4,4]]]])
   torch.cat((a0,a1),dim=3).type(torch.FloatTensor)
   tensor([[[[1., 1., 1., 1., 3., 3., 3., 3.],
             [2., 2., 2., 2., 4., 4., 4., 4.]]]])
   torch.Size([1, 1, 2, 8])
   ```

   

- 通过卷积层学习图片的空间信息；

  通过池化层降低图片信息敏感度；

   通过全连接层转换到类别空间。

- [self.__class__.__name__](https://blog.csdn.net/Yy_Rose/article/details/123549727?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168051294616800182757522%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168051294616800182757522&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-123549727-null-null.142^v81^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=.__class__.__name__&spm=1018.2226.3001.4187)

  **作用：**获取当前类的类名

  **原理：**首先用 __class__ 将 self实例变量指向类，然后再去调用 __name__ 类属性

- torch.numel()

  ```python
  t = torch.randn((2,3,4))
  t.numel()
  #24
  ```

- 要学习的参数个数：

每个卷积层参数：co*ci*kh*kw+co(bias),全连接层参数就多了，卷积后图像拉直送入全连接层

- [**下采样和上采样**](https://blog.csdn.net/ytusdc/article/details/121452878?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168057663916800211584736%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168057663916800211584736&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121452878-null-null.142^v81^insert_down38,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%E4%B8%8A%E9%87%87%E6%A0%B7%E5%92%8C%E4%B8%8B%E9%87%87%E6%A0%B7&spm=1018.2226.3001.4187)

  1、下采样的作用是什么？通常有哪些方式？
  	下采样层有两个作用，一是减少计算量，防止过拟合；二是增大感受野，使得后面的卷积核能够学到更加全局的信息。

  下采样的方式主要有两种：
  		1、采用stride为2的池化层，如Max-pooling和Average-pooling，目前通常使用Max-pooling，因为他计算简单而且能够更好的保留纹理特征；
  		2、采用stride为2的卷积层，下采样的过程是一个信息损失的过程，而池化层是不可学习的，用stride为2的可学习卷积层来代替pooling可以得到更好的效果，当然同时也增加了一定的计算量。

  2、上采样的原理和常用方式

  在卷积神经网络中，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(如图像的语义分割)，这个使图像由小分辨率映射到大分辨率的操作，叫做上采样，它的实现一般有三种方式：

  ​		1、插值，一般使用的是双线性插值，因为效果最好，虽然计算上比其他插值方式复杂，但是相对于卷积计算可以说不值一提，其他插值方式还有最近邻插值、三线性插值等；

  ​		2、转置卷积又或是说反卷积(Transpose Conv)，通过对输入feature map间隔填充0，再进行标准的卷积计算，可以使得输出feature map的尺寸比输入更大；是一种可以学习的向上采样方式，里面的参数可以学习

  nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')
  		3、Up-Pooling - Max Unpooling && Avg Unpooling --Max Unpooling，在对称的max pooling位置记录最大值的索引位置，然后在unpooling阶段时将对应的值放置到原先最大值位置，其余位置补0；没有参数，速度更快，采取给定策略上采样

- **nn.AdaptiveAvgPool2d**

  AdaptivePooling，自适应池化层。使用这种池化方式，核（kernal）和步长（stride）是函数根据输入的原始尺寸、目标尺寸**自动计算**出来的。

  ```python
  *# 自适应最大池化* 
  
  out=nn.AdaptiveAvgPool2d(output_size=100)
  ```

  构造模型的时候，AdaptiveAvgPool2d()的位置一般在卷积层和全连接层的交汇处，以便确定输出到Linear层的大小。

- 卷积核的组数是输出通道，每组有多少个卷积核是输入通道

- 批量归一化：保证层间输出和梯度符合某一特定分布，以提高数据和损失的稳定性

- 网络常用设计思路：

  高宽减半，通道加倍

- nn.Conv2d和F.conv2d

- ```python
  nn.Conv2d(in_chanels,out_chanels,kernel_size=3,stride=1,padding=0)
  x=(batch,in_chanels,n_h,n_w)
  out=(batch,out_chanels,m_h,m_w)
  w=(out_chanels,int_chanels,k_h,k_w)
  b=(out_chanels)
  
  F.conv2d(x,w,b,stride=,padding=) 
  #x,w,b,y的结构都是一样的
  ```

  ![image](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520315.jpeg)

​					**图片中的偏差B是C0，不是C0*Ci**

- 卷积核的输出形状计算公式：

  <img src="D:\AppData\Typora\typora-user-images\image-20230407154849459.png" alt="image-20230407154849459" style="zoom: 200%;" />

- torch.mm只适用于二维矩阵，而torch.matmul可以适用于高维

- 图像增广：翻转，切割，变色

- detach() 、detach_()和 data 的区别

  1）detach()与detach_()

  ```
  在x->y->z传播中，如果我们对y进行detach()，梯度还是能正常传播的，但如果我们对y进行detach_()，就把x->y->z切成两部分：x和y->z，x就无法接受到后面传过来的梯度
  
  2）detach()和data
  
  共同点：x.data（x.detach()） 返回和 x 的相同数据 tensor, 这个新的tensor和原来的tensor（即x）是共用数据的，一者改变，另一者也会跟着改变，且require s_grad = False
  
  不同点： x.data 不能被 autograd 追踪求微分，即使被改了也能错误求导，而x.detach()则不行
  ```

- [torchvision.datasets.ImageFolder](https://blog.csdn.net/weixin_54546190/article/details/126123675)

  ```python
  train_imgs=torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))
  print(train_imgs.classes)
  print(train_imgs.class_to_idx)
  print(train_imgs[0])#结构是(img_data, class_id),带标签
  print(train_imgs[0][0])#是img_data
  
  #结果：
  ['hotdog', 'not-hotdog']
  {'hotdog': 0, 'not-hotdog': 1}
  (<PIL.Image.Image image mode=RGB size=122x144 at 0x2844AFD7648>, 0)
  <PIL.Image.Image image mode=RGB size=122x144 at 0x2844B716808>
  ```

- [训练集、验证集与测试集](https://blog.csdn.net/mooyuan/article/details/123544318)

训练集（train set） —— 用于训练模型（拟合参数）：即模型拟合的数据样本集合，如通过训练拟合一些参数来建立一个分类器。

验证集（validation set）—— 用于确定网络结构或者控制模型复杂程度的超参数（拟合超参数）：是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。 通常用来在模型迭代训练时，用以验证当前模型泛化能力（准确率，召回率等），防止过你话的现象出现，以决定如何调整超参数。具体原理参照本文的二（三）。

测试集（test set） —— 用来评估模最终模型的性能如何（评价模型好坏）：测试集没有参于训练，主要是测试训练好的模型的准确能力等，但不能作为调参、选择特征等算法相关的选择的依据。说白了就只用于评价模型好坏的一个数据集。

- [torch.optim.lr_scheduler.StepLR](https://blog.csdn.net/weixin_44543648/article/details/122623774)

  学习率调整

  ```python
  torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=- 1, verbose=False)
  
  #optimizer：要优化的优化器
  #step_size：每训练step_size个epoch，更新一次参数
  #gamma：更新lr的乘法因子
  ```

  ![在这里插入图片描述](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520687.png)

- **torch.set_printoptions**(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)

  **修改输出精度**

  precision是每一个元素的输出精度，默认是八位；
  threshold是输出时的阈值，当tensor中元素的个数大于该值时，进行缩略输出，默认时1000；
  edgeitems是输出的维度，默认是3；
  linewidth字面意思，每一行输出的长度；
  profile=None，修正默认设置

- opencv,PIL,numpy和torch读取数据格式及转化

**1.PIL的Image.open()读取图片**

PIL图像在转换为numpy.ndarray后，格式为(h,w,c)，像素顺序为RGB**；**

**2.opencv的imread读取图像**

OpenCV在cv2.imread()后数据类型为numpy.ndarray，格式为(h,w,c)，像素顺序为BGR；

**3.torchvision.transforms.ToTensor()**

**源码说明：将PIL image或者一个numpy.ndarray变成tensor**

**当PIL image在**(L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)之间

当numpy.adarray类型为np.uint8

**输入应该是numpy.ndarray,维度是（H,W,C）,数值【0-255】**

**输出变成torch.FloatTensor,维度是（C,H,W）,数值在【0.0-1.0】之间**

可以用permute函数进行维度转化

```py
ndarray=>tensor   torchvision.transforms.ToTensor()  
tensor=>ndarray   tensor.permute(0,2,3,1)
```

- [torch.meshgrid(*tensors,indexing=None)](https://blog.csdn.net/flyingluohaipeng/article/details/125033606?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168153781316800182179206%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168153781316800182179206&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-125033606-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=torch.meshgrid&spm=1018.2226.3001.4187)

函数的功能是生成网格，可以用于生成坐标。

1输入两个一维tensor数据，且两个tensor数据类型相同

2输出两个tensor数据

3参数indexing是可选的，可选值为’‘xy’‘或’‘ij’‘，默认为’‘ij’'。

如果选择**“ij”**，则输出两个tensor维度的行列个数与输入的两个tensor的元素个数分别对应相同，如上面例子所示。
如果选择’**‘xy’'**，则输出两个tensor的第一个维度对应于第二个输入的元素个数，第二个维度对应于第一个输入的元素个数。其中第一个输出张量填充第一个输入张量中的元素，各列元素相同；第二个输出张量填充第二个输入张量中的元素，各行元素相同，与indexing='‘ij’'输出结果情况相反

```python
x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5])
grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')
print(x)
print(y)
print(grid_x)
print(grid_y)
结果如下：
tensor([1, 2, 3])
tensor([4, 5])
tensor([[1, 1],
        [2, 2],
        [3, 3]])
tensor([[4, 5],
        [4, 5],
        [4, 5]])
```

```python
x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5])
grid_x, grid_y = torch.meshgrid(x, y, indexing='xy')
print(x)
print(y)
print(grid_x)
print(grid_y)
结果如下所示：
tensor([1, 2, 3])
tensor([4, 5])
tensor([[1, 2, 3],
        [1, 2, 3]])
tensor([[4, 4, 4],
        [5, 5, 5]])
```

- [pytorch中repeat()函数](https://blog.csdn.net/tequila53/article/details/119183678?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168154229116800188516713%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168154229116800188516713&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-119183678-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=repeat%E5%87%BD%E6%95%B0&spm=1018.2226.3001.4187)

对数组进行重复。

```python
a = torch.tensor([[1, 2, 3],
                  [1, 2, 3]])
b = a.repeat(2, 2)
print(b.shape)
print(b)
=>torch.Size([4, 6])
tensor([[1, 2, 3, 1, 2, 3],
        [1, 2, 3, 1, 2, 3],
        [1, 2, 3, 1, 2, 3],
        [1, 2, 3, 1, 2, 3]])
#repeat的参数是对应维度的复制个数，上段代码为0维复制两次，1维复制两次，则得到以上运行结果。其余扩展情况依此类推


# a形状(2,3)
a = torch.tensor([[1, 2, 3],
                  [1, 2, 3]])
# repeat参数比维度多，在扩展前先讲a的形状扩展为(1,2,3)然后复制
b = a.repeat(1, 2, 1)
print(b.shape)  # 得到结果torch.Size([1, 4, 3])
```

- [张量复制方法repeat、repeat_interleave和tile](https://blog.csdn.net/bqw18744018044/article/details/127481387?ops_request_misc=&request_id=&biz_id=102&utm_term=repeat_interleave&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-127481387.142^v83^insert_down38,239^v2^insert_chatgpt&spm=1018.2226.3001.4187)

- [torch.unsqueeze()和torch.squeeze()](https://blog.csdn.net/flyingluohaipeng/article/details/125092937?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168154439916800225519146%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168154439916800225519146&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-125092937-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=unsqueeze&spm=1018.2226.3001.4187)

`torch.squeeze(input, dim=None, out=None)` 

squeeze()函数的功能是维度压缩。返回一个tensor（张量），其中 input 中维度大小为1的所有维都已删除。
举个例子：如果 input 的形状为 (A×1×B×C×1×D)，那么返回的tensor的形状则为 (A×B×C×D)
当给定 dim 时，那么只在给定的维度（dimension）上进行压缩操作，注意给定的维度大小必须是1，否则不能进行压缩。
`torch.unsqueeze(input, dim) → Tensor`

unsqueeze()函数起升维的作用,参数dim表示在哪个地方加一个维度，注意dim范围在:[-input.dim() - 1, input.dim() + 1]之间，比如输入input是一维，则dim=0时数据为行方向扩，dim=1时为列方向扩，再大错误。

- [torch.max()](https://blog.csdn.net/flyingluohaipeng/article/details/125093651?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168156584316800222862877%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168156584316800222862877&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-125093651-null-null.142^v83^insert_down38,239^v2^insert_chatgpt&utm_term=torch.max&spm=1018.2226.3001.4187)同torch.min

1.torch.max(input)

```python
1.输入一维张量，返回一维张量里面最大值
x = torch.randn(4)
y = torch.max(x)
x,y
输出结果如下：
(tensor([-0.6223,  0.0043, -0.8753,  1.4240]), tensor(1.4240))

2 输入二维张量，返回二维张量里面最大值
x = torch.randn(3,4)
y = torch.max(x)
x,y
输出结果如下：
(tensor([[-1.1052,  0.1026,  0.9994, -0.3092],
         [-0.8400,  0.2004,  0.9212,  0.7807],
         [-1.2979, -0.4327,  2.3044,  0.0140]]),
 tensor(2.3044))

3 输入两个一维张量，输出这两个张量里面相应元素中的最大值
x = torch.randn(4)
z = torch.randn(4)
max = torch.max(x,z)
x,z,max
输出结果如下：
(tensor([-1.5147, -1.2790, -1.0159, -0.4732]),
 tensor([-0.4547, -2.8545,  0.0554, -0.3548]),
 tensor([-0.4547, -1.2790,  0.0554, -0.3548]))

4 输入两个张量，一个张量一维，一个张量二维，此时一维张量会进行广播成二维张量，然后再输出这两个张量里面相应元素中的最大值，输出张量为二维。
x = torch.randn(3,4)
z = torch.randn(4)
max = torch.max(x,z)
x,z,max
输出结果如下：
(tensor([[ 1.1917,  0.6338,  0.7590, -0.9802],
         [ 0.2247,  0.3635,  1.3743,  1.6229],
         [ 1.6165,  0.0634,  0.5259,  0.1285]]),
 tensor([3.4765, 0.4480, 0.1502, 0.3738]),
 tensor([[3.4765, 0.6338, 0.7590, 0.3738],
         [3.4765, 0.4480, 1.3743, 1.6229],
         [3.4765, 0.4480, 0.5259, 0.3738]]))

5 输入两个二维张量，输出这两个张量里面相应元素中的最大值，输出张量为二维。
x = torch.randn(3,4)
z = torch.randn(3,4)
max = torch.max(x,z)
x,z,max
输出结果如下：
(tensor([[-0.0835,  0.0718, -1.7404, -0.3218],
         [ 0.0577,  0.6271,  1.4014, -0.6417],
         [ 0.3917,  0.0761,  1.2479, -0.4352]]),
 tensor([[-0.0717,  0.3822,  0.7256,  1.4147],
         [-0.1271,  0.1503,  0.3934,  1.6760],
         [-2.2341,  2.5286, -0.3500, -0.1751]]),
 tensor([[-0.0717,  0.3822,  0.7256,  1.4147],
         [ 0.0577,  0.6271,  1.4014,  1.6760],
         [ 0.3917,  2.5286,  1.2479, -0.1751]]))
```

2.torch.max(input,dim=x)

输入input（二维）张量，

当dim=0时表示找出每列的最大值，函数会返回两个tensor，第一个tensor是每列的最大值，第二个tensor是每列最大值的索引；

当dim=1时表示找出每行的最大值，函数会返回两个tensor，第一个tensor是每行的最大值；第二个tensor是每行最大值的索引。

```python
x = torch.randn(3,4)
max,indices = torch.max(x,dim=1)
x,max,indices
输出结果如下：
(tensor([[ 1.4832,  0.1886, -0.3044, -0.6111],
         [-0.8998,  0.0610,  0.3388,  1.7176],
         [ 1.6153,  0.6864,  2.3225,  1.3818]]),
 tensor([1.4832, 1.7176, 2.3225]),
 tensor([0, 3, 2]))
```

- [torch.argmax()](https://blog.csdn.net/flyingluohaipeng/article/details/125099214)

1.`torch.argmax(input) → LongTensor`

**将输入input张量，无论有几维，首先将其reshape排列成一个一维向量，然后找出这个一维向量里面最大值的索引。**

```python
import torch
x = torch.randn(3,4)
y = torch.argmax(x)#对应于x中最大元素的索引值
x,y
(tensor([[-0.84,  1.66,  0.60, -0.49],
         [ 0.89,  1.16,  1.40, -0.45],
         [-0.83,  0.06,  0.26,  0.67]]),
 tensor(1))
```

2.`torch.argmax(input, dim, keepdim=False) → LongTensor`

**函数返回其他所有维在这个维度上面张量最大值的索引。**

```python
import torch
x = torch.randn(3,4)
y = torch.argmax(x,dim=0)#dim=0表示将dim=0这个维度大小由3压缩成1，然后找到dim=0这三个值中最大值的索引，这个索引表示dim=0行索引标号
x,x.shape,y,y.shape
输出结果如下：
(tensor([[ 2.6347,  0.6456, -1.0461, -1.5154],
         [-1.3955, -1.2618, -0.5886, -0.5947],
         [-1.5272, -2.0960,  0.9428, -0.9532]]),
 torch.Size([3, 4]),
 tensor([0, 0, 2, 1]),
 torch.Size([4]))
```

- [torch.sort()和torch.argsort()](https://blog.csdn.net/flyingluohaipeng/article/details/125093568)

1.torch.sort()

`torch.sort(input, dim=- 1, descending=False, stable=False, *, out=None)`

**输入input，在dim维进行排序，默认是dim=-1对最后一维进行排序，descending表示是否按降序排,默认为False，输出排序后的值以及对应值在原输入imput中的下标**

```python
x = torch.randn(3, 4)
sorted, indices = torch.sort(x)
x,sorted,indices
输出结果如下：
(tensor([[-1.3864,  0.5811, -0.1056, -0.3237],
         [-0.2136, -1.4806,  0.4986,  0.9382],
         [-0.2820,  0.1171, -0.3983, -0.8061]]),
 tensor([[-1.3864, -0.3237, -0.1056,  0.5811],
         [-1.4806, -0.2136,  0.4986,  0.9382],
         [-0.8061, -0.3983, -0.2820,  0.1171]]),
 tensor([[0, 3, 2, 1],
         [1, 0, 2, 3],
         [3, 2, 0, 1]]))
```

2.torch.argsort()

`torch.argsort(input,dim=-1,descending=False)`

- pytorch clamp()

在最大值和最小值之间对input进行裁剪.

```python
torch.clamp(input, min, max, out=None)→ Tensor
or a.clamp(min,max)
```

![image-20230415223050119](https://cdn.jsdelivr.net/gh/caozihao1205/blog_img/img/202306041520133.png)

- [[:,None]用于增加维度](https://blog.csdn.net/flyingluohaipeng/article/details/125093683?spm=1001.2014.3001.5506)

```python
#三维变四维
a=torch.tensor([[[1,2,3],
                 [4,5,6]],
   				[[1,2,3],
                 [4,5,6]]])
print(a[None,:,:,:])
print(a[:,None,:,:])#None在第一维和第三维之间就扩展第二维
print(a[:,:,None,:])
print(a[:,:,:,None])
=>
tensor([[[[1, 2, 3],
          [4, 5, 6]],

         [[1, 2, 3],
          [4, 5, 6]]]])
tensor([[[[1, 2, 3],
          [4, 5, 6]]],


        [[[1, 2, 3],
          [4, 5, 6]]]])
tensor([[[[1, 2, 3]],

         [[4, 5, 6]]],


        [[[1, 2, 3]],

         [[4, 5, 6]]]])
tensor([[[[1],
          [2],
          [3]],

         [[4],
          [5],
          [6]]],


        [[[1],
          [2],
          [3]],

         [[4],
          [5],
          [6]]]])
```

```python
#二维变三维，广播机制
a=torch.tensor([[1,2,3,4],
                [4,5,6,7]])#(2,4)
b=torch.tensor([[1,2,3,4],
                [5,6,7,8],
                [5,6,7,8]])#(3,4)
print(a[None,:,:])#(1,2,4)
print(a[:,None,:])#(2,1,4)
print(a[:,:,None])#(2,4,1)
print(torch.max(a[:,None,:],b))#(2,3,4)
print(a[:,None,0])#二维(2,1)
=>
tensor([[[1, 2, 3, 4],
         [4, 5, 6, 7]]])
tensor([[[1, 2, 3, 4]],

        [[4, 5, 6, 7]]])
tensor([[[1],
         [2],
         [3],
         [4]],

        [[4],
         [5],
         [6],
         [7]]])
tensor([[[1, 2, 3, 4],
         [5, 6, 7, 8],
         [5, 6, 7, 8]],

        [[4, 5, 6, 7],
         [5, 6, 7, 8],
         [5, 6, 7, 8]]])
tensor([[1],
        [4]])
```

- [交并比手写公式推导（IoU）](https://blog.csdn.net/weixin_52366977/article/details/127109929?spm=1001.2014.3001.5506)

- torch.full()

```python
torch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor
```

**返回创建size大小的维度，里面元素全部填充为fill_value**

```python
x = torch.full(size=(2,3),fill_value=5)
x
输出结果如下：
tensor([[5, 5, 5],
        [5, 5, 5]])
```

- torch.nonzero()

1.输入是一维张量，返回一个包含输入 input 中非零元素索引的张量，输出张量中的每行包含 input 中非零元素的索引，输出是二维张量torch.size(z,1)， z 是输入张量 input 中所有非零元素的个数。

2.输入是n维张量，如果输入 input 有 n 维,则输出的索引张量的size为torch.size(z,n) , 这里 z 是输入张量 input 中所有非零元素的个数。
**无论输入是几维，输出张量都是两维，每行代表输入张量中非零元素的索引位置(在所有维度上面的位置)。**
**返回 input中非零元素的索引下标，n维input 中的元素的索引有n 个维度的索引下标。**

```python
x = torch.tensor([1, 1, 1, 0, 1])
y = torch.nonzero(x)
x,x.shape,y,y.shape
输出结果如下：
(tensor([1, 1, 1, 0, 1]),
 torch.Size([5]),
 tensor([[0],
         [1],
         [2],
         [4]]),
 torch.Size([4, 1]))


x = torch.tensor([[0.6, 0.0, 0.9, 0.0],
                  [0.0, 0.4, 0.0, 0.0],                            
                  [0.0, 0.0, 1.2, 0.0],
                  [0.0, 0.7, 0.0,-0.4]])
y = torch.nonzero(x)
x,x.shape,y,y.shape
输出结果如下：
(tensor([[ 0.6000,  0.0000,  0.9000,  0.0000],
         [ 0.0000,  0.4000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  1.2000,  0.0000],
         [ 0.0000,  0.7000,  0.0000, -0.4000]]),
 torch.Size([4, 4]),
 tensor([[0, 0],
         [0, 2],
         [1, 1],
         [2, 2],
         [3, 1],
         [3, 3]]),
 torch.Size([6, 2]))


x = torch.randn(2,3,4)
y = (x>0.1)
z = torch.nonzero(y)
x,x.shape,y,y.shape,z,z.shape
输出结果如下：
(tensor([[[ 0.3326, -0.9972, -0.4871, -1.3885],
          [ 0.4679, -1.7913,  2.0604,  0.3150],
          [-0.6156, -0.5204,  0.2902, -0.0780]],
 
         [[ 1.2206, -0.7150, -0.1662,  0.5120],
          [ 0.2907,  0.1285,  0.8520, -1.2698],
          [ 0.5176, -0.3800,  0.4408,  0.5073]]]),
 torch.Size([2, 3, 4]),
 tensor([[[ True, False, False, False],
          [ True, False,  True,  True],
          [False, False,  True, False]],
 
         [[ True, False, False,  True],
          [ True,  True,  True, False],
          [ True, False,  True,  True]]]),
 torch.Size([2, 3, 4]),
 tensor([[0, 0, 0],
         [0, 1, 0],
         [0, 1, 2],
         [0, 1, 3],
         [0, 2, 2],
         [1, 0, 0],
         [1, 0, 3],
         [1, 1, 0],
         [1, 1, 1],
         [1, 1, 2],
         [1, 2, 0],
         [1, 2, 2],
         [1, 2, 3]]),
 torch.Size([13, 3]))
```

- torch.long()

取整数部分

```python
import torch
x = torch.randn(3,4)
x,x.long()
=>(tensor([[-0.09, -0.55,  1.13,  0.04],
         [ 1.19,  2.01,  0.91,  1.59],
         [-1.11,  0.27, -0.69,  2.03]]),
 tensor([[ 0,  0,  1,  0],
         [ 1,  2,  0,  1],
         [-1,  0,  0,  2]]))
```

